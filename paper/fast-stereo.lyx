#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url}
%\usepackage{natbib}
\usepackage[authoryear]{natbib}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Task-Driven Resource Allocation for Real-Time Stereo with Belief Propagation
\end_layout

\begin_layout Author
Eric Hunsberger, Jeff Orchard & Bryan Tripp
\end_layout

\begin_layout Abstract
Stereo matching is a versatile approach to depth estimation.
 However, accurate algorithms for dense stereo do not run at frame rates
 that are practical for robotics.
 In this paper we explore an approach that focuses computational resources
 for depth estimation on important parts of the scene.
 A coarse analysis is performed over the whole image, and a more accurate
 analysis is performed over a small selected region that we call the 
\begin_inset Quotes eld
\end_inset

fovea
\begin_inset Quotes erd
\end_inset

.
 The fovea is moved in each frame to minimize a cost function that reflects
 uncertainty in the depth image.
 Importantly, parameters of the cost function, such as relative weights
 of central vs.
 peripheral parts of the scene, and of underestimates vs.
 overestimates, can be adjusted to suit the task.
 For example, a suitable cost function for grasp control might be squared
 error in the neighbourhood of the target object, while a suitable cost
 function for navigation might weigh overestimates of depth more heavily
 than underestimates, as they are more likely to result in collisions.
 The fovea is moved in each frame to minimize this cost, and its movement
 is therefore affected by the task-dependent parameters of the cost function.
 We evaluate this approach on stereo videos from the KITTI Vision Benchmark
 Suite, and show that it improves depth estimates within a real-time performance
 constraint.
 We discuss analogies with top-down, task-dependent control of human eye
 movements in humans.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Knowledge of distances to surfaces in the environment is essential for robot
 navigation, manipulation, etc.
 Stereoscopic disparity, i.e.
 difference in the location of a given feature in images from two offset
 cameras, is a rich source of this information.
 Disparity is inversely related to the feature's distance (depth) from the
 cameras.
 Compared to other approaches to depth estimation, stereo is currently more
 cost effective than LIDAR, and it can provide estimates from a large field
 of view at high higher frame rates.
 In contrast with systems that rely on patterned infrared illumination (e.g.
 the Microsoft Kinect), stereo matching is also viable in a wide range of
 indoor and outdoor lighting conditions.
\end_layout

\begin_layout Standard
The first step in stereo depth estimation is to match image features in
 one camera with those in the other camera.
 This is done by comparing a small region of one image with a range of horizonta
lly-offset regions in the other image.
 The most similar region in the second image, or perhaps one of the few
 most similar regions, is more likely than others to correspond to the same
 point in physical space.
 Sparse stereo methods match only a minority of features, particularly those
 that are distinctive and therefore likely to be matched correctly [TODO:
 ref].
 They do not estimate depth in regions with more ambiguous features.
 In contrast, dense methods provide estimates over the whole field of view,
 even in featureless regions.
 Dense methods are particularly useful for many robotics applications, because
 surface locations may be important whether or not they are filled with
 distinctive visual features.
 However, in the absence of distinctive features (e.g.
 on an image of a textureless wall), a wide range of regions may match equally
 well.
 Information from other, more distinctive parts of the image, must somehow
 be taken into account to resolve the ambiguity.
\end_layout

\begin_layout Standard
A successful approach has been to model the disparity map as a Markov random
 field [TODO: ref], in which neighbouring disparities are highly correlated.
 Given this model, if an ambiguous region (with a broad probability density
 over disparities) is surrounded by unambiguous regions (with narrow probability
 densities), then its ambiguity is resolved by the spatial correlations.
 Given a typical natural image pair with varied ambiguity in different parts,
 the maximum a posteriori depth image can be approximated from the Markov
 random field model in various ways, including loopy belief propagation
 (ref), graph cuts (ref), and Markov Chain Monte Carlo (ref) methods.
 However, despite many recent advances in efficiency, these methods remain
 computationally intensive.
 This limits these methods' practicality for robots, which require high
 frame rates with modest computational resources.
 
\end_layout

\begin_layout Standard
This study takes a biologically inspired approach to work around this problem.
 The human visual system uses disparity as a depth cue, and the brain only
 has enough neurons to process a small fraction of the visual field in detail
 at any given time.
 A disproportionately large amount of cortex is dedicated to the centre
 of the visual field (the fovea), with progressively fewer cortical resources
 available for more peripheral regions of the visual field, a concept known
 as cortical magnification 
\begin_inset CommandInset citation
LatexCommand cite
key "Daniel1961"

\end_inset

.
 The eyes move frequently, typically jumping to a new target several times
 per second, to sense and analyze detailed information from different parts
 of the scene in series.
 These rapid eye movements (saccades) are sometimes directed to salient
 visual features, such as the onset of motion.
 However, importantly, the eyes are most often directed in a task-dependent
 manner, e.g.
 to the next set of letters, while reading 
\begin_inset CommandInset citation
LatexCommand cite
key "Rayner2010"

\end_inset

, or to the edge of an obstacle [TODO: ref], or to something the person
 intends to pick up 
\begin_inset CommandInset citation
LatexCommand cite
key "Johansson2001"

\end_inset

.
 The visual target can even be a completely featureless region, as when
 a person glances at a spot on a table which is indistinct except that the
 person means to put something there [TODO: ref].
 
\end_layout

\begin_layout Standard
Taking inspiration from the primate visual system, our goal was to obtain
 practical real-time stereo depth estimates by allocating computational
 resources to the most task-relevant sub-images in each frame.
 We defined task-specific cost functions that weigh squared estimation errors
 according to their location, magnitude, and sign (i.e.
 whether the error is an overestimate or underestimate).
 Given such a cost function, our system estimates the cost at each pixel,
 and moves a 
\begin_inset Quotes eld
\end_inset

fovea
\begin_inset Quotes erd
\end_inset

 (a small window in which accurate, computationally intensive analysis is
 performed) to the image region with highest cost.
 We show that this approach allows improved estimation of obstacle distances
 at practical frame rates.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
Our general strategy is to perform fast disparity estimation over most of
 the image, and more accurate estimation in a small window, and to move
 this window each frame to minimize a task-specific cost function.
 This general strategy could be used with a variety of stereo algorithms,
 but we focus here on a specific implementation of the strategy that uses
 loopy belief propagation.
 
\end_layout

\begin_layout Subsection
Markov Random Fields and Loopy Belief Propagation
\end_layout

\begin_layout Standard
Due to ambiguities in pixel-by-pixel correspondences across image pairs,
 disparity estimation is more accurate when it takes into account statistical
 relationships between disparities at different image positions.
 Useful disparity images have tens of thousands of pixels or more, so it
 is not practical to model the full correlation matrix.
 A popular and successful approach [TODO: refs] has been to model disparity
 images as Markov random fields.
 In a Markov random field, each region is independent of the rest of the
 field, conditional on values in the region's boundary.
 That is, 
\begin_inset Formula $p(x_{i}|x_{b},x_{o})=p(x_{i}|x_{b})$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is the field inside the boundary, 
\begin_inset Formula $x_{b}$
\end_inset

 is the part of the field that makes up the boundary, and 
\begin_inset Formula $x_{o}$
\end_inset

 is the field outside the boundary [TODO: ref Fieguth].
 A variety of numerical approaches exist for estimating the maximum a posteriori
 disparity image given uncertain pixel-by-pixel estimates.
 
\end_layout

\begin_layout Standard
One such numerical approach is belief propagation (BP), an algorithm for
 inference on graphical models, inlcuding Bayesian networks and Markov random
 fields.
 BP produces exact solutions on trees [TODO: ref].
 The statistical dependencies in a Markov random field contain loops, and
 these prevent exact inference.
 However, BP typically produces good approximations after running for a
 few iterations [TODO: ref].
 
\end_layout

\begin_layout Standard
[TODO: introduce BP in terms of probabilities]
\end_layout

\begin_layout Standard
The algorithm minimizes a sum of two cost terms, called the data cost and
 the discontinuity cost.
 In stereo estimation, the data cost is evaluated as a function of pixel
 locations 
\begin_inset Formula $x,y$
\end_inset

 in the left image and disparity 
\begin_inset Formula $d$
\end_inset

 of offset pixels in the right image.
 The data cost increases with increasing difference in appearance between
 a neighbourhood around pixel 
\begin_inset Formula $x,y$
\end_inset

 in the left image and a neighbourhood around pixel 
\begin_inset Formula $x-d,y$
\end_inset

 in the right image.
 We use a neighbourhood of one pixel, because larger neighbourhoods tend
 not to produce much more accurate results after belief propagation [TODO:
 ref].
 Specifically, if 
\begin_inset Formula $I^{l}$
\end_inset

 and 
\begin_inset Formula $I^{r}$
\end_inset

 are 2D luminance images from the left and right cameras, then the data
 cost is 
\begin_inset Formula 
\[
C_{x,y,d,k}=\min\left\{ \left|I_{x,y,k}^{l}-I_{x-d,y,k}^{r}\right|,C_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are the horizontal and vertical image coordinates, respectively, 
\begin_inset Formula $d$
\end_inset

 is the disparity (in pixels), 
\begin_inset Formula $C_{\mathrm{max}}$
\end_inset

 is a saturation value, and 
\begin_inset Formula $k$
\end_inset

 is the frame index.
 
\end_layout

\begin_layout Standard
We used an implementation of loopy BP [TODO: ref Felzenszwalb & Huttenlocher]
 with several optimizations that provide orders-of-magnitude acceleration.
 The most salient of these in the present context is a multiscale approach
 that reduces the iterations needed to propagate information to distant
 parts of the image.
 The first step in this process is the construction of a data cost pyramid.
 Beginning with the full cost volume, a coarser volume is created, half
 the size in each dimension, by summing the costs over 2x2 neighbourhoods
 of pixels.
 This is then repeated with on the coarser volume, and so on for several
 levels, producing coarser levels of the pyramid.
 [TODO: describe multiscale messages]
\end_layout

\begin_layout Standard
Disparity is estimated after belief propagation as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{x,y,k}^{est}=\operatorname{arg\,min}_{d}\,C_{x,y,d,k}.
\]

\end_inset


\end_layout

\begin_layout Standard
[TODO: Laplacian]
\end_layout

\begin_layout Subsection
System Overview
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:motivation"

\end_inset

 illustrates the motivation for our approach.
 The two curves show error in stereo disparity estimation with different
 image resolutions.
 The lower-resolution images (left curve) are downsampled by a factor of
 two in each dimension, relative to the higher-resolution images.
 Downsampling decreases runtime and increases error.
 For a given resolution, error decreases with increasing numbers of iterations
 of belief propagation (BP), but most of this decrease occurs in the first
 few iterations.
 Beyond 5-10 iterations, error can only be reduced substantially further
 by switching to a higher resolution (e.g.
 by switching from the left curve to the right curve).
 
\end_layout

\begin_layout Standard
Suppose we have a certain time budget per frame (e.g.
 1/4s) and the right curve is entirely outside it.
 In this case, we can only afford to process the full frame at lower resolution.
 However, it may be possible to reduce error somewhat below the left curve,
 by processing only part of each frame at higher resolution.
 This part can be whatever size uses all the available time.
 Furthermore, in many applications (e.g.
 navigation, grasping), some areas in each frame are likely to be more important
 than others.
 If the most important areas are processed at higher resolution, then the
 runtime may remain acceptable while the importance-weighted error approaches
 that of higher-resolution BP.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-rationale.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multiscale belief propagation on example data, illustrating the motivation
 for our approach.
 Disparity estimation error is plotted vs.
 runtime.
 The right curve (circles) were obtained with high-resolution images, and
 the left curve (squares; higher error) with lower-resoluton versions of
 the same images.
 Within each curve, the runtime varies due to varying numbers of iterations
 at each scale (1, 2, 3, 4, 5, 7, 10, and 15 iterations).
 Given a time budget of, for example, 0.25s/frame, it would not be possible
 to process these images at high resolution.
 However, if certain areas in the images are of greater practical interest,
 then results that are nearly as useful might be achieved by processing
 just those areas at high resolution.
 The data are 25 frames taken from the KITTI dataset [TODO: ref], downsampled
 by a factor of two (high resolution) and four (low resolution) in each
 dimension.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:motivation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The system performs multiscale belief propagation (BP) with different numbers
 of scales in different parts of the image.
 The number of operations in BP scales as [TODO].
 Therefore most of the computational cost is incurred at the finest scales.
 However, our system only processes the finest scale at the fovea.
 This results in depth estimates that are similar to that of full BP in
 the fovea (which we direct to task-relevant regions), with a speedup of
 the same order as we would get by omitting the finest scale.
 
\end_layout

\begin_layout Standard
Importantly, messages from coarser scales are used at the finest scale in
 the fovea.
 This makes depth estimates in the fovea continuous with those in the surroundin
g areas, and also makes them similar to those of full belief propagation
 (differing due to fine-scale propagation across the border in full BP).
 [TODO: describe efficient data cost and other differences this creates]
 We also experimented with running multiscale BP independently in the fovea,
 and promoting continuity with the surround in other ways, such as copying
 message messages from the surround into adjacent outer pixels of the fovea.
 However, these approaches were less effective because they do not allow
 information from coarse scales to propagate from other image regions across
 the fovea.
 
\end_layout

\begin_layout Standard
Figure 1 illustrates the system architecture.
 The inputs to the system in each frame are 1) a rectified stereo pair of
 luminance images, 2) the position of the fovea, and 3) parameters of the
 cost function (described in the next section).
 The latter would normally be static throughout a task, but this is not
 a requirement.
 The outputs after processing each frame are 1) a disparity map, and 2)
 the position of the fovea for the next frame.
\end_layout

\begin_layout Standard
Downsampled copies of the disparity estimate 
\begin_inset Formula $D_{k}$
\end_inset

 are stored for several frames to allow integration of information over
 time.
 Frames 
\begin_inset Formula $D_{k-n},...,D_{k}$
\end_inset

 are used to produce estimates of both disparity and its uncertainty for
 frame 
\begin_inset Formula $k$
\end_inset

.
 We sometimes retain only the foveal regions of past frames rather than
 full images.
 We define task-specific 
\begin_inset Quotes eld
\end_inset

uncertainty cost
\begin_inset Quotes erd
\end_inset

 functions that weigh the cost of disparity under- and over-estimates in
 various parts of the scene.
 After each frame is processed, the system reports the disparity map that
 minimizes this cost, and also moves the fovea to the region with the highest
 cost.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename system-sketch.pdf
	scale 60
	BoundingBox 130bp 150bp 590bp 390bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
System architecture.
 The 2D boxes correspond to single-channel luminance and disparity images.
 The thicker boxes correspond to image variables with more channels, including
 the data cost over multiple disparities (~100 channel images) and disparity
 history over multiple frames (~5 channels).
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Remapping Past Frames
\end_layout

\begin_layout Standard
If the cameras are moving, we project 
\begin_inset Formula $D_{k-n},...,D_{k}$
\end_inset

 into the current frame's coordinates prior to choosing the fovea position.
 To do this, first a previous frame's disparity estimate is mapped to depth
 estimate, then a 3D location estimate in the left camera's previous coordinate
 system, then the corresponding estimate in the left camera's new coordinate
 system, and finally into a disparity image from the approximate perspective
 of the new camera position.
 
\end_layout

\begin_layout Standard
Due to changes in perspective, there is not a one-to-one map between pixels
 in the previous and new disparity images, so some pixels in the remapped
 disparity image are blank.
 We experimented with two ways of filling in the blank pixels.
 The first was linear interpolation, and the second was setting blank pixels
 to zero and taking the maximum over a few neighbouring pixels.
 Some pixels also corresponded to multiple previous disparities per frame
 rather than zero or one.
 [TODO: For these pixels, the largest disparity is chosen (corresponding
 to the nearest surface).] Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:interp"

\end_inset

 compares these two methods.
 Taking the maximum was much faster than bilinear interpolation (XXs vs.
 XXs), with comparable results, so we used the former method.
 
\end_layout

\begin_layout Standard
[TODO: use whole frame as seed with lower weight?]
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/smudge-vs-interp.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of interpolation methods for remapped data.
 The top left panel is a disparity map 
\begin_inset Formula $D_{k}^{est}$
\end_inset

.
 The top right is the previous frame's estimate, 
\begin_inset Formula $D_{k-1}^{est}$
\end_inset

, remapped into the current frame's perspective.
 Aside from the missing data (dark blue points) this is a fair approximation
 of the current disparity estimate, because while the cameras are moving,
 nothing else in the scene is moving.
 The bottom right panel shows linear interpolation of the remapped data.
 The bottom left panel is the maximum of the remapped image, and copies
 of this image shifted one pixel vertically and one pixel both left and
 right.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:interp"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Using Information from Past Frames in Disparity Estimates
\end_layout

\begin_layout Standard
If information is not propagated across the image (e.g.
 through BP) then dense stereo estimates typically have large errors in
 many small patches, corresponding to spurious correlations.
 BP greatly reduces such errors, but it does not remove them completely.
 Further improvements are possible by propagating information over time,
 across a sequence of frames, so that each pixel has neighbours in time
 as well as horizontally and vertically [TODO: ref].
 This information can be useful if for example certain features have been
 more distinct in previous frames due to differences in viewpoint.
 However, this process is expensive in terms of runtime and memory.
 We sought to inexpensively approximate this approach by adding the cost
 term,
\begin_inset Formula 
\[
S_{x,y,d,k}=\min\left\{ \sum_{i-1}^{n}\left|d-D_{x,y,k-i}\right|,S_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $S_{max}$
\end_inset

 is the level at which this cost term saturates.
 Saturation prevents excessive penalization of legitimate large differences
 between frames, for example when something in the environment moves.
 This approach has much lower runtime and storage costs, because fewer messages
 are needed, and messages from previous frames need not be stored or remapped
 into new coordinates.
 However, we also wanted to avoid the cost of storing and remapping past
 disparities [TODO: how long does full-resolution remapping take?].
 To further reduce these costs, we stored only the foveal regions of previous
 disparity images (as these were expected to be the most accurate), and
 we downsampled them.
 With these changes, the method [TODO: comment on runtime.]
\end_layout

\begin_layout Standard
This method also allows the algorithm to ignore information from past frames
 if the data-cost function is unambiguous.
 This is important, because predictions from past frames can be less accurate
 than current estimates, e.g.
 due to extrinsic motion in the environment.
 Certain other ways of efficiently using information from past frames (e.g.
 low-pass filtering the depth, or using a cost term or prior probability
 without saturation) do not have this advantage.
\end_layout

\begin_layout Standard
[TODO: 
\emph on
I don't think we're using any of this any more
\emph default
.
 At step 
\begin_inset Formula $k$
\end_inset

, both the depth image and uncertainty about the depth image are estimated
 using data from image pairs 
\begin_inset Formula $k-n$
\end_inset

 to 
\begin_inset Formula $k$
\end_inset

.
 
\end_layout

\begin_layout Standard
The probability density 
\begin_inset Formula $p_{x,y,k}$
\end_inset

 over disparity at each pixel is modelled as a mixture of Gaussians, 
\begin_inset Formula 
\[
p_{x,y,k}(d)\approx\sum_{i=0}^{n}w_{x,y,i}G(\mu_{x,y,i},\sigma_{x,y,i},d),
\]

\end_inset

where 
\begin_inset Formula $w_{x,y,i}$
\end_inset

 is a mixing weight, and 
\begin_inset Formula $\mu_{x,y,i}$
\end_inset

 and 
\begin_inset Formula $\sigma_{x,y,i}$
\end_inset

 are the Gaussian mean and standard deviation, respectively.
 We set 
\begin_inset Formula 
\[
w_{x,y,i}=\frac{\sigma_{x,y,i}^{-1}}{\sum_{j=0}^{n}\sigma_{x,y,j}^{-1}}.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
For pixels that are missing from previous frames due to perspective changes,
 we set 
\begin_inset Formula $\sigma_{x,y,i}^{-1}=0$
\end_inset

.
 
\end_layout

\begin_layout Standard
The means of the Gaussian components are 
\begin_inset Formula $\mu_{x,y,i}=D_{x,y,k-i}$
\end_inset

, i.e.
 the remapped disparity estimates.
 For each new frame, 
\begin_inset Formula $\sigma_{x,y,0}$
\end_inset

 is set to a small value (
\begin_inset Formula $\sigma_{0}^{f}$
\end_inset

) for pixels in the fovea, and a larger value (
\begin_inset Formula $\sigma_{0}^{p}$
\end_inset

) for pixels in the periphery.
 This reflects the lower expected accuracy of peripheral pixels.
 These initial widths are parameters of the system (see Table
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:parameters"

\end_inset

).
 
\end_layout

\begin_layout Standard
The Gaussian widths increase with each step as 
\begin_inset Formula $\sigma_{x,y,i+1}=\sigma_{x,y,i}+\sigma_{\mathrm{step}}(\mu_{x,y,i})$
\end_inset

, where 
\begin_inset Formula $\sigma_{\mathrm{step}}$
\end_inset

 models increasing uncertainty as the observation ages, due to possible
 motion of surfaces in the environment.
 We assume that such extrinsic motion is statistically uniform as a function
 of 
\emph on
depth
\emph default
, rather than disparity.
 We define 
\begin_inset Formula $\sigma_{\mathrm{step}}^{z}$
\end_inset

 as the expected change in depth 
\begin_inset Formula $z$
\end_inset

 due to extrinsic motion.
 For small angles of disparity, depth and disparity are inversely related
 as 
\begin_inset Formula $d\approx\alpha/z$
\end_inset

, where 
\begin_inset Formula $\alpha$
\end_inset

 is a constant due to the distance between cameras and the number of pixels
 per unit horizontal angle.
 The increase in Gaussian width in 
\emph on
disparity
\emph default
 space is therefore 
\begin_inset Formula 
\[
\sigma_{\mathrm{step}}(\mu_{x,y,i})=\frac{\partial d}{\partial z}\sigma_{\mathrm{step}}^{z}\approx\frac{\alpha}{z^{2}}\sigma_{\mathrm{step}}^{z}\approx\frac{d^{2}}{\alpha}\sigma_{\mathrm{step}}^{z}\approx\frac{(\mu_{x,y,i})^{2}}{\alpha}\sigma_{\mathrm{step}}^{z},
\]

\end_inset

where 
\begin_inset Formula $\mu_{x,y,i}$
\end_inset

 is the expected disparity.] 
\end_layout

\begin_layout Standard
[TODO: explain that we use gaussians because BP log probability is not very
 accurate (also maybe try using it).]
\end_layout

\begin_layout Subsection
Fovea Placement
\end_layout

\begin_layout Standard
- integral image of cost including importance - also just importance - describe
 importance measure - describe a hypothetical alternative importance calculation
 for grasping based on Platt's method with low-res stereo first
\end_layout

\begin_layout Standard
In each frame, the system tries to place the fovea in order to minimize
 a task-specific cost function.
 The cost functions consider disparity estimation error and also the frame-by-fr
ame relevance of different image regions to the task.
 The simplest cost we use is the unweighted squared error, 
\begin_inset Formula 
\[
C_{x,y}^{u}=(D_{x,y}^{est}-D_{x,y})^{2},
\]

\end_inset

where 
\begin_inset Formula $D_{x,y}$
\end_inset

 is ground-truth disparity.
 In various contexts we used three variations of 
\begin_inset Quotes eld
\end_inset

ground truth
\begin_inset Quotes erd
\end_inset

.
 First, we used ground truth that is provided with the Kitti dataset for
 selected isolated frames.
 Second, we used LIDAR data, which is available for all video frames.
 Its limitations are that it is somewhat sparser than the images, it does
 not cover the upper visual field, and (importantly) it is not synchronized
 with the camera sensors.
 The latter limiation introduces large errors around the edges of moving
 objects.
 Our final variation of 
\begin_inset Quotes eld
\end_inset

ground truth
\begin_inset Quotes erd
\end_inset

 was high-resolution BP with 50 iterations.
 This was expected to be somewhat inaccurate, but more accurate on average
 than our faster approaches.
 
\end_layout

\begin_layout Standard
We also used importance-weighted cost, 
\begin_inset Formula 
\[
C_{x,y}^{w}=w_{x,y}(D_{x,y}^{est}-D_{x,y})^{2},
\]

\end_inset

where the weight 
\begin_inset Formula $w_{x,y}$
\end_inset

 emphasizes task-critical regions.
 A wide variety of weighting schemes are possible.
 One obvious scheme for navigation is to emphasize regions in the direction
 of travel, because the risk of collisions with obstacles is greatest in
 this direction.
 Another is to emphasize regions in which surfaces are relatively close
 (i.e.
 disparity is high).
 Before processing each frame, we downsampled the previous frame's disparity
 estimate by a factor of eight in each dimension, and projected it into
 the current perspective.
 We will refer to this remapped disparity as 
\begin_inset Formula $D_{x,y}^{m}$
\end_inset

.
 This provided an initial estimate of the disparities in different parts
 of the image that was useful for fovea placement, with minimal computational
 cost due to downsampling.
 
\end_layout

\begin_layout Standard
Weighting by proximity emphasizes the ground in front of the car, which
 is always close, even though it is typically clear of obstacles.
 Instead, we used the following proximity weight: 
\begin_inset Formula 
\[
w_{x,y}^{p}=\max(D_{x,y}^{m}-\bar{D}_{x,y},0),
\]

\end_inset

where 
\begin_inset Formula $\bar{D}_{x,y}$
\end_inset

 is the mean disparity at location 
\begin_inset Formula $x,y$
\end_inset

.
 This emphasizes parts of the image in which surfaces are closer than usual.
 We combined this frame-by-frame weight with a static weight template 
\begin_inset Formula $w_{x,y}^{s}$
\end_inset

, which was highest in the horizontal centre of the image (the direction
 of travel) and also lower at the top of the image than the bottom.
 Figure XX shows an example frame in which a cyclist is strongly emphasized
 by this method.
 
\end_layout

\begin_layout Standard
We calculated 
\begin_inset Formula $\bar{D}_{x,y}$
\end_inset

 over entire videos before processing, which would be unrealistic in the
 field.
 However, a recursive low-pass filter with a long time constant would have
 a similar effect with low computational burden.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/importance.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Weighting by disparity in excess of the average.
 Top: Average disparity 
\begin_inset Formula $\bar{D}_{x,y}$
\end_inset

 estimated over a long image sequence.
 Center: Disparity 
\begin_inset Formula $D_{x,y}$
\end_inset

 estimated in a single frame.
 Bottom: The weight 
\begin_inset Formula $w^{s}w^{p}$
\end_inset

, where 
\begin_inset Formula $w^{s}$
\end_inset

 is a static weight template that emphasizes the direction of travel, and
 
\begin_inset Formula $w_{x,y}^{p}=\max(D_{x,y}^{m}-\bar{D}_{x,y},0)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
[TODO: discuss also weighting by uncertainty?]
\end_layout

\begin_layout Standard
relevant image regions that are is deviate have top of the visual fionly
 covers the bottom wWe approximated least-squareWe denote the uncertainty
 at for each pixel 
\begin_inset Formula $(x,y)$
\end_inset

 in each image 
\begin_inset Formula $k$
\end_inset

 as 
\begin_inset Formula $U_{x,y,k}$
\end_inset

.
 The simplest is the squared-error uncertainty cost,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
U_{x,y,k}=\int p_{x,y,k}(\delta)\left(d_{x,y}^{\mathrm{est}}-\delta\right)^{2}d\delta,
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
where 
\begin_inset Formula $d_{x,y}^{\mathrm{est}}$
\end_inset

 is the estimated disparity at pixel 
\begin_inset Formula $(x,y)$
\end_inset

.
 In some cases, we might want to use an asymmetric cost, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
U_{x,y,k}=\int p_{x,y,k}(\delta)\left(w_{\mathrm{over}}\left[d_{x,y}^{\mathrm{est}}-\delta\right]_{+}^{2}+w_{\mathrm{under}}\left[\delta-d_{x,y}^{\mathrm{est}}\right]_{+}^{2}\right)d\delta,
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align left
where 
\begin_inset Formula $[\,]_{+}$
\end_inset

 indicates half-wave rectification, 
\begin_inset Formula $w_{\mathrm{over}}$
\end_inset

 is the weight of overestimates of the disparity, and 
\begin_inset Formula $w_{\mathrm{under}}$
\end_inset

 is the weight of underestimates.
 In addition to these spatial uncertainty cost functions, we can evaluate
 the uncertainty of an entire image using a weighted sum over the uncertainties
 in the image,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
U_{k}=\sum_{x,y}w_{x,y}U_{x,y,k},
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
where 
\begin_inset Formula $w_{x,y}$
\end_inset

 weights some image locations more heavily than others in a task-dependent
 way (for example, in the direction of travel).
\end_layout

\begin_layout Standard
The system outputs the disparity map that minimizes one of these costs (dependin
g on the task) at each pixel.
 In the symmetric case, the per-pixel cost can be decomposed as 
\begin_inset Formula 
\[
U_{x,y,k}=\sum_{i=0}^{n}w_{x,y,i}\left[\sigma_{x,y,i}+(d_{x,y}^{\mathrm{est}}-\mu_{x,y,i})^{2}\right]
\]

\end_inset

and the minimum is simply the weighted average of the Gaussian means, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
U_{x,y,k}=\frac{\sum_{i=0}^{n}w_{x,y,i}\mu_{x,y,i}}{\sum_{i=0}^{n}w_{x,y,i}}.
\]

\end_inset

We are not aware of a tractable expression for the minimum in the asymmetric
 case, and numerical evaluation of this minimum at each pixel is impractical.
 However, the cost can be closely approximated as, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
U_{x,y,k} & = & \sum_{i=0}^{n}w_{x,y,i}\Big(w_{\mathrm{under}}m\left[\sigma_{x,y,i}+(d_{x,y}^{\mathrm{est}}-\mu_{x,y,i})^{2}\right]+\nonumber \\
 &  & \qquad\qquad\quad w_{\mathrm{over}}(1-m)\left[\sigma_{x,y,i}+(d_{x,y}^{\mathrm{est}}-\mu_{x,y,i})^{2}\right]\Big),\label{eq:cost_function_approx}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula 
\[
m=\frac{1}{1+\exp(-\pi(d_{x,y}^{\mathrm{est}}-\mu_{x,y,i})/\sigma_{x,y,i})}.
\]

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cost_function_approx"

\end_inset

 illustrates this approximation.
 This leads to the approximate minumum, [TODO], which is inexpensive to
 calculate.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename asymmetric cost approx.eps
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of our approximation of asymmetric cost.
 In this case, squared underestimates of disparity (overestimates of depth)
 are weighed five times as heavily as squared errors of the opposite sign.
 This asymmetry is reasonable in navigation, because the cost of collision
 is higher than the cost of keeping an over-conservative distance.
 The actual cost is plotted with circles, and the approximation of Eq.
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:cost_function_approx"

\end_inset

 is plotted as a line.
 The differences between actual and approximate cost in this plot are less
 than 0.01.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:cost_function_approx"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The uncertainty cost 
\begin_inset Formula $U_{x,y.k}$
\end_inset

 is also used to select the next location of the fovea.
 The systems forms an integral image of 
\begin_inset Formula $U$
\end_inset

, with integration limits corresponding to the size of the fovea.
 Then the bottom right corner of the fovea is set to the pixel with the
 maximum integrated cost.
 Because disparities are highly correlated across adjacent frames, and 
\begin_inset Formula $\sigma_{0}^{f}<\sigma_{0}^{p}$
\end_inset

, this is expected to reduce the uncertainty cost.]
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
The methods were tested on data from the KITTI Vision Benchmark Suite [TODO:
 ref].
 This dataset includes high-resolution rectified stereo images (1242x375
 pixels) taken from the roof of a car during city driving.
 In addition to isolated stereo images, the dataset includes a number of
 video sequences, supporting methods (such as ours) which propagate information
 over time.
 Furthermore, ground-truth depth is available from a LIDAR sensor for parts
 of the image.
 [TODO: how do we deal with time offsets] The scenes include static obstacles
 such as buildings and parked cars, and moving obstacles such as cars, pedestria
ns, and people on bicycles.
 
\end_layout

\begin_layout Subsection
Parameters
\end_layout

\begin_layout Standard
The system parameters and their values for different tests are listed in
 Table
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:parameters"

\end_inset

.
 Optimal parameters were found using the package Hyperopt 
\begin_inset CommandInset citation
LatexCommand cite
key "Bergstra2013"

\end_inset

.
 Hyperopt attempts to remove the irreproducible ``art'' of hand-tuning hyperpara
meters, instead using an automated method for searching hyperparameter space
 to find optimal (or near-optimal) values.
 [TODO: hyperopt procedure on full BP for ground truth and on our method]
\end_layout

\begin_layout Standard
Table XX lists the parameters that we used for our performance tests (see
 section XX).
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
System parameters and values used in performance tests.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of disparities
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Area of fovea
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
X x X pixels
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Frame history length
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BP iterations per scale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data cost saturation value (
\begin_inset Formula $C_{max}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Discontintuity cost saturation value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Peripheral # scales
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Foveal # scales
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
History downsampling factor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:parameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

 shows two disparity estimates from a single frame, with the fovea in different
 places (the centres are marked with white + signs).
 These examples were processed with multiple levels of resolution, with
 a small high-resolution region in the centre and multiple levels of decreasing
 resolution with greater distance from the centre, analogous to the human
 eye and visual cortex.
 The total runtime of BP in these examples is only a few times as great
 as BP at the lowest resolution.
 In the top frame, the fovea clearly resolves the cyclist, while in the
 bottom frame the fovea correctly interprets a low-disparity region in which
 the top frame has an artefact.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-examples.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of foveal multiscale belief propagation.
 The white + marks indicate fovea centres.
 These examples were processed at multiple resolutions, with equal computational
 demands at each of these resolutions (e.g.
 a parafoveal region twice the size of the fovea had half the resolution).
 In the top panel, the fovea resolves a cyclist at high resolution.
 There is a large artefact in the top-centre of the image.
 In the bottom panel (same frame), the fovea is instead centred on this
 artefact, and corrects it.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:fovea-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:seed-example"

\end_inset

 shows an example of the disparity estimation that is improved through the
 use of information from a previous frame.
 The top panel is the same as that of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

.
 It contains an artefact in the sky, which is featureless in this scene
 and therfore has ambiguous disparity.
 The centre panel is a remapped disparity estimate from a foveal and parafoveal
 region in the previous frame, overlapping with the artefact in the top
 centre.
 The bottom panel shows a new estimate (with the fovea on the cyclist, similar
 to that of the top panel) but with an additional cost term that penalizes
 deviations from the remapped estimate in the centre panel.
 This example illustrates that information from previous frames has the
 potential to improve disparity estimates, as one would expect due to correlatio
ns over time.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/seed-vs-no-seed.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of the use of information from a previous frame to improve disparity
 estimation.
 Top: Estimate with fovea on cyclist, as the top panel of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

.
 Centre: A remapped estimate from the fovea of the previous frame, in which
 the fovea was in the top centre of the image.
 Bottom: New disparity estimate with fovea on the cyclist, but also using
 the prior estimate from the centre panel.
 The prior estimate resolves the ambiguity in the current frame, and corrects
 the artefact.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:seed-example"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
- [TODO: flops per frame]
\end_layout

\begin_layout Standard
- [TODO: examples and averages of least-squared error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: examples of asymmetric error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: examples of asymmetric weighted error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: example for grasping with another dataset]
\end_layout

\begin_layout Standard
- [TODO: frame rate]
\end_layout

\begin_layout Standard
- [TODO: error vs.
 somewhat less coarse at same frame rate]
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
[TODO: summarize main results]
\end_layout

\begin_layout Standard
[TODO: discuss additional computation and memory needed vs.
 BP computation]
\end_layout

\begin_layout Standard
[TODO: emphasize that this would probably work with other stereo methods
 too, maybe even local ones at low/high resolution on low-end hardware (although
 would the rest of the system dominate runtime?)]
\end_layout

\begin_layout Standard
Our system focuses resources on different parts of the scene by moving an
 analysis window.
 An alternative approach is to move the cameras (refs).
 This approach might be particularly useful with yoked foveal and peripheral
 cameras that have different fields of view (the foveal one having a narrower
 field of view and finer detail).
 However, this approach would involve further cost and complexity, inlcuding
 four actuators to move each camera assembly in two degrees of freedom.
 Actuation at the speeds required to move the fovea frequently is not straightfo
rward.
 In human saccades, rotational acceleration can exceed 
\begin_inset Formula $20,000{}^{\circ}/s^{2}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Abrams1989"

\end_inset

.
 A recent system that achieved appropriate speeds (ref Villgrattner) used
 actuators that cost thousands of dollars each.
 
\end_layout

\begin_layout Subsection
Future Work
\end_layout

\begin_layout Standard
This approach is suited to navigation in mobile robots with limited processing
 power.
 We are specifically intersted in adapting this system for a panoramic catadoptr
ic stereo sensor that is in development, and testing it in navigation among
 pedestrians.
 This may allow further fine-tuning of the importance weights.
 For example the weights could be more directly tied to decisions that the
 robot must make, e.g.
 whether a narrow gap is too narrow to navigate.
 
\end_layout

\begin_layout Standard
Another potential direction for future work would be to adapt the method
 to parallel hardware such as a GPU or FPGA.
 The multiple-resolution version of our approach (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

) is well suited for such an implementation, because each scale has the
 same number of pixels.
 For example, if the fovea is 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels, then the immediate parafoveal region is 
\begin_inset Formula $2m$
\end_inset

 by 
\begin_inset Formula $2n$
\end_inset

, downsampled by a factor of two, for a total of 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels.
 An FPGA with a capacity for 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

 pixels could process a frame in a few steps, potentially yielding very
 high framerates with modest hardware.
 
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Section*
Acknowledgements 
\end_layout

\begin_layout Standard
This work was supported in part by a Mitacs and CrossWing Inc.
 The authors thank John-Paul Gignac of CrossWing Inc.
 for helpful discussions.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "fast-stereo"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
