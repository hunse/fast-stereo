#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url}
%\usepackage{natbib}
\usepackage[authoryear]{natbib}
\DeclareMathOperator*{\argmin}{arg\,min}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Task-Driven Resource Allocation for Real-Time Stereo with Belief Propagation
\end_layout

\begin_layout Author
Eric Hunsberger, Jeff Orchard & Bryan Tripp
\end_layout

\begin_layout Abstract
Stereo matching is a versatile approach to depth estimation.
 However, accurate algorithms for dense stereo do not run at frame rates
 that are practical for robotics.
 In this paper we explore an approach that focuses computational resources
 for depth estimation on important parts of the scene.
 Specifically, we use multiscale loopy belief propagation, but only run
 the finest scales in a small selected region that we call the 
\begin_inset Quotes eld
\end_inset

fovea
\begin_inset Quotes erd
\end_inset

 (as its purpose is analogous to the high-resolution region of the human
 retina).
 The fovea is moved in each frame to minimize a task-dependent cost function.
 Its movement is therefore determined by a combination of task parameters
 and image features.
 We evaluate this approach on stereo videos from the KITTI Vision Benchmark
 Suite, using a simple cost function that emphasizes pixels with higher-than-nor
mal disparity for their image location.
 We show that our approach improves depth estimates within a real-time performan
ce constraint.
 We discuss analogies with task-dependent control of human eye movements.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Information about the locations of surfaces in the environment is essential
 for robot navigation, manipulation, etc.
 Stereoscopic disparity (i.e.
 difference in the location of a given feature in images from two offset
 cameras) is a rich source of this information.
 Disparity is inversely related to the feature's distance (depth) from the
 cameras.
 Compared to other approaches to depth estimation, stereo is currently more
 cost effective than LIDAR, and it can provide estimates from a large field
 of view at high frame rates.
 It is also more economical and higher-resolution than time-of-flight devices.
 In contrast with systems that rely on patterned infrared illumination (e.g.
 the Microsoft Kinect), it is not limited by infrared interference from
 sunlight.
 
\end_layout

\begin_layout Standard
The first step in stereo depth estimation is to match image features in
 one camera with those in the other camera.
 This is done by comparing a small region of one image with a range of horizonta
lly-offset regions in the other image.
 The most similar region in the second image (or perhaps one of the few
 most similar regions) is relatively likely to correspond to the same point
 in physical space.
 
\emph on
Sparse
\emph default
 stereo methods match only a minority of features, i.e.
 distinctive ones that are likely to be matched correctly [TODO: ref].
 In contrast, 
\emph on
dense
\emph default
 methods provide estimates over the whole field of view, even in featureless
 regions.
 Dense methods are of interest in robotics, because surface locations may
 affect the robot whether or not they are filled with distinctive visual
 features.
 However, in the absence of distinctive features (e.g.
 in an image of a textureless wall), a wide range of regions may match equally
 well.
 
\end_layout

\begin_layout Standard
Neighbouring disparities are highly correlated except at object boundaries.
 If an ambiguous region (with a broad probability density function spread
 over a wide range of disparities) is close to unambiguous regions (with
 narrow probability densities), then accounting for spatial correlations
 can profoundly improve estimates in the ambiguous region.
 An effective approach is to model the disparity map as a Markov random
 field [TODO: ref].
 Then, maximum 
\emph on
a posteriori
\emph default
 depth images can be estimated using various methods, such as loopy belief
 propagation and graph cuts [Tappen & Freeman, 2003; Szeliski et al., 2008].
 But despite many recent advances in efficiency, these methods remain computatio
nally intensive, limiting their practicality for robots, which require high
 frame rates with modest computational resources.
 
\end_layout

\begin_layout Standard
In this study we explore the benefits of processing different parts of the
 scene in different levels of detail.
 This general strategy is based on the primate visual systems.
 Primates use disparity as a depth cue, but in contrast with conventional
 image processing, a disproportionately large amount of cortex is dedicated
 to the centre of the visual field (the fovea), with progressively fewer
 cortical resources available for more peripheral parts of the visual field
 
\begin_inset CommandInset citation
LatexCommand cite
key "Daniel1961"

\end_inset

.
 The eyes move frequently, typically jumping to a new target several times
 per second, to sense and analyze detail from different parts of the scene
 in series.
 Analogously, in this study we process 
\begin_inset Quotes eld
\end_inset

foveal
\begin_inset Quotes erd
\end_inset

 image regions at high resolution and 
\begin_inset Quotes eld
\end_inset

peripheral
\begin_inset Quotes erd
\end_inset

 regions at lower resolution in order to save computation time.
 
\end_layout

\begin_layout Standard
The key problem is where to position the fovea in each frame.
 Primates address this problem through sophisticated systems for directing
 visual attention and eye movements (reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Kowler2011,Hayhoe2005,Schutz2011"

\end_inset

).
 Computational models of these systems, and applications to computer vision
 and robotics, are reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Borji2013,Frintrop2010,Kimura2013"

\end_inset

[also Tsotsos (2011)].
 As discussed in these reviews, human eye movements are occasionally directed
 to salient visual features (such as the onset of motion) but in most situations
 are overwhelmingly determined by task demands.
 Examples of task-dependent targets include the next word while reading
 
\begin_inset CommandInset citation
LatexCommand cite
key "Rayner2010"

\end_inset

, the edge of an obstacle while walking (Rothkopf & Ballard, 2007), an object
 a person wants to pick up 
\begin_inset CommandInset citation
LatexCommand cite
key "Johansson2001"

\end_inset

, etc.
 The visual target can even be a completely featureless region.
 For example people often glance at a spot on a table where they intend
 to put something, even though this spot may be visually indistinct [TODO:
 ref].
 However, eye movements are very often determined by a combination of bottom-up
 and top-down factors.
 
\end_layout

\begin_layout Standard
A simple example of bottom-up / top-down interaction occurs in visual search
 tasks.
 Viewing an image of many small shapes, humans can rapidly find shapes with
 a distinctive feature (e.g.
 the yellow ones; the horizontal ones; etc.) Interestingly, visual search
 for more complex conjunctions of features (e.g.
 horizontal yellow shapes) is more slower and less automatic 
\begin_inset CommandInset citation
LatexCommand cite
key "Treisman1980"

\end_inset

.
\end_layout

\begin_layout Standard
Taking inspiration from the primate visual system, our goal here is to obtain
 practical real-time stereo depth estimates by allocating computational
 resources to the most task-relevant image regions in each frame.
 Taking further inspiration from the visual search literature, we direct
 the fovea using a simple visual feature that is relevant to visual navigation.
 Specifically, we use previous frames to estimate an expected diparity map,
 and define an 
\begin_inset Quotes eld
\end_inset

importance map
\begin_inset Quotes erd
\end_inset

 as the half-rectified difference between expected disparity and a time-average
 disparity (see Maki et al., 2000, for another use of depth as an attention
 cue).
 This allows us to direct the fovea to regions in which surfaces seem to
 be closer than normal for their part of the visual field, which often correspon
d to obstacles.
 We show that this approach leads to improved estimation of obstacle surfaces
 at practical frame rates.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
Our general strategy is to perform fast disparity estimation over most of
 the image, and more accurate estimation in a small window, and to move
 this window each frame to minimize a task-specific cost function.
 This general strategy could be used with a variety of stereo algorithms,
 but we focus here on a specific implementation of the strategy that uses
 loopy belief propagation.
 
\end_layout

\begin_layout Subsection
Markov Random Fields and Loopy Belief Propagation
\end_layout

\begin_layout Standard
Pixel-by-pixel stereo correspondances are frequently ambiguous, so disparity
 estimation is improved by considering spatial correlations.
 The full correlation matrix is unmanageable, becaused disparity images
 of practical interest have tens of thousands of pixels or more.
 A successful approach (e.g.
 Sun et al., 2003) has been to model disparity images as Markov random fields.
 In a Markov random field (MRF), each region is independent of the rest
 of the field, conditional on values in the region's boundary.
 That is, 
\begin_inset Formula $p(x_{i}|x_{b},x_{o})=p(x_{i}|x_{b})$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is the part of the field inside the boundary, 
\begin_inset Formula $x_{b}$
\end_inset

 is the part that comprises the boundary, and 
\begin_inset Formula $x_{o}$
\end_inset

 is the part outside the boundary [TODO: ref Fieguth].
 There are various ways to estimate the maximum 
\emph on
a posteriori
\emph default
 disparity from the MRF.
 
\end_layout

\begin_layout Standard
One such method is belief propagation (BP), a general algorithm for inference
 on graphical models (inlcuding Bayesian networks as well as MRFs).
 BP provides exact solutions on trees.
 Loops in the statistical relationships of Markov random fields prevent
 exact inference, but 
\begin_inset Quotes eld
\end_inset

loopy
\begin_inset Quotes erd
\end_inset

 BP typically produces good approximations after running for a few iterations
 [TODO: ref Murphy et al., 1999].
 
\end_layout

\begin_layout Standard
The starting point for our work is a BP implementation (Felzenszwalb & Huttenloc
her, 2006) that has several optimizations, which together accelerate the
 algorithm by several orders of magnitude.
 One of these optimizations is a multiscale method that reduces the numbers
 of iterations needed to propagate information to distant parts of the image.
 Our modification consists simply of executing finer scales (which take
 up most of the computation time) only in sub-images rather than over the
 whole image.
 
\end_layout

\begin_layout Standard
The BP implementation of (Felzenszwalb & Huttenlocher, 2006) results in
 a labelling 
\begin_inset Formula $f$
\end_inset

, which assigns a label 
\begin_inset Formula $f_{p}\in\mathcal{L}$
\end_inset

 to pixel 
\begin_inset Formula $p$
\end_inset

, where 
\begin_inset Formula $\mathcal{L}$
\end_inset

 consists of all possible disparities.
 The algorithm minimizes an energy function 
\begin_inset Formula 
\[
E(f)=\sum_{p\in\mathcal{P}}D_{p}(f_{p})+\sum_{p,q\in\mathcal{N}}V(f_{p}-f_{q})
\]

\end_inset

where 
\begin_inset Formula $D_{p}$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

data cost
\begin_inset Quotes erd
\end_inset

) is the cost of labelling pixel 
\begin_inset Formula $p$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

, and 
\begin_inset Formula $V(f_{p}-f_{q})$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

discontinuity cost
\begin_inset Quotes erd
\end_inset

) is an additional cost of labelling neighbouring pixels 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

 and 
\begin_inset Formula $f_{q}$
\end_inset

.
 Because the disparity of neighbouring pixels is strongly correlated, a
 larger discontinuity cost is assigned for larger differences between neighbours.
 Minimizing 
\begin_inset Formula $E(f)$
\end_inset

 corresponds to maximum 
\emph on
a posteriori
\emph default
 estimation in a probabilistic context (Felzenszwalb & Huttenlocher, 2006).
 
\end_layout

\begin_layout Standard
BP involves iterative computation and exhange of messages between pixels.
 In iteration 
\begin_inset Formula $t$
\end_inset

, the message 
\begin_inset Formula $m_{p\to q}^{t}$
\end_inset

 from pixel 
\begin_inset Formula $p$
\end_inset

 to pixel 
\begin_inset Formula $q$
\end_inset

 is (Felzenszwalb & Huttenlocher, 2006)
\begin_inset Formula 
\[
m_{p\to q}^{t}=min_{f_{p}}\left(V(f_{p}-f_{q})+D_{p}(f_{p})+\sum_{s\in\mathcal{N}(p)\backslash q}m_{s\to p}^{t-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\mathcal{N}(p)\backslash q$
\end_inset

 consists of the neighbours of 
\begin_inset Formula $p$
\end_inset

 other than 
\begin_inset Formula $q$
\end_inset

.
 After 
\begin_inset Formula $T$
\end_inset

 iterations, final labels 
\begin_inset Formula $f_{p}^{*}$
\end_inset

 are assigned as 
\begin_inset Formula 
\[
f_{p}^{*}=\argmin_{f_{p}}D(f_{p})+\sum_{q\in\mathcal{N}(p)}m_{q\to p}^{T}(f_{p}).
\]

\end_inset

 
\end_layout

\begin_layout Standard
In stereo estimation, the first step in this process is to calculate a data
 cost volume in terms of image coordinates 
\begin_inset Formula $p=x,y$
\end_inset

 (where 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are the horizontal and vertical image coordinates, respectively), and disparity
 (in pixels) 
\begin_inset Formula $d$
\end_inset

.
 The data cost volume is specifically 
\begin_inset Formula 
\[
D_{x,y,d}=\min\left\{ \left|I_{x,y}^{l}-I_{x-d,y}^{r}\right|,D_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $I^{l}$
\end_inset

 and 
\begin_inset Formula $I^{r}$
\end_inset

 are luminance of the left and right images, and 
\begin_inset Formula $D_{\mathrm{max}}$
\end_inset

 is a saturation value that limits the cost of large discontinuities.
 The saturation value is used because while most discontinuities are expected
 to be small, some (at object boundaries) are expected to be larger, with
 weak expectations about how much larger.
 
\end_layout

\begin_layout Standard
Prior to calculating the data cost, we processed the images with a Laplacian
 filter, in order to emphasize edges.
 
\end_layout

\begin_layout Standard
We refer to the disparity estimated from BP in frame 
\begin_inset Formula $k$
\end_inset

 as 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 .
\end_layout

\begin_layout Subsection
System Overview
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:motivation"

\end_inset

 illustrates the motivation for our approach.
 The two curves show error in stereo disparity estimation with different
 image resolutions.
 The lower-resolution images (left curve) are downsampled by a factor of
 two in each dimension, relative to the higher-resolution images.
 Downsampling decreases runtime and increases error.
 For a given resolution, error decreases with increasing numbers of iterations
 of belief propagation (BP), but most of this decrease occurs in the first
 few iterations.
 Beyond 5-10 iterations, error can only be reduced substantially further
 by switching to a higher resolution (e.g.
 by switching from the left curve to the right curve).
 
\end_layout

\begin_layout Standard
Suppose we have a certain time budget per frame (e.g.
 1/4s) and the right curve is entirely outside it.
 In this case, we can only afford to process the full frame at lower resolution.
 However, it may be possible to reduce error somewhat below the left curve,
 by processing only part of each frame at higher resolution.
 This part can be whatever size uses all the available time.
 Furthermore, in many applications (e.g.
 navigation, grasping), some areas in each frame are likely to be more important
 than others.
 If the most important areas are processed at higher resolution, then the
 runtime may remain acceptable while the importance-weighted error approaches
 that of higher-resolution BP.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-rationale.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Performance of multiscale belief propagation on example data, illustrating
 the motivation for our approach.
 Disparity estimation error is plotted vs.
 runtime.
 The right curve (circles) were obtained with a high-resolution stereo image
 pair, and the left curve (squares; higher error) with lower-resoluton versions
 of the same images.
 Within each curve, the runtime varies with numbers of iterations at each
 scale (1, 2, 3, 4, 5, 7, 10, and 15 iterations).
 Given a time budget of, for example, 0.25s/frame, it would not be possible
 to process these images at high resolution.
 However, if certain areas in the images were of greater practical interest,
 then results that are nearly as useful might be achieved by processing
 just those areas at high resolution.
 The data are 25 frames taken from the KITTI dataset [TODO: ref], downsampled
 by a factor of two (high resolution) and four (low resolution) in each
 dimension.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:motivation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The system performs multiscale belief propagation (BP) with different numbers
 of scales in different parts of the image.
 The number of operations in a single scale of Felzenszwalb & Huttenlocher's
 method is order 
\begin_inset Formula $O(nl)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of pixels and 
\begin_inset Formula $l$
\end_inset

 is the number of disparities.
 The majority of the computational cost is incurred at the finest scale,
 which has four times as many pixels as the next-finest scale.
 Our system only processes the finest one or two scales in the foveal region.
 The resulting depth estimate is similar to that of full BP in the fovea,
 with nearly the same speedup one would get by omitting the finest scale(s).
 
\end_layout

\begin_layout Standard
Importantly, messages from coarser scales are used to initialize messages
 in fine foveal scales.
 This makes depth estimates in the fovea continuous with those in the surroundin
g areas, and also makes them similar to those of full belief propagation.
 (They differ somewhat around the fovea border, due to propagation across
 the border in the fine scale of full BP.) We also experimented with running
 multiscale BP independently in the fovea, and promoting continuity with
 the surround in other ways, such as copying message messages from the surround
 into adjacent outer pixels of the fovea.
 However, these approaches were less effective because they did not allow
 information from coarse scales to propagate from other image regions across
 the fovea.
 
\end_layout

\begin_layout Standard
Figure 1 illustrates our general approach.
 The inputs to the system in each frame are 1) a rectified stereo pair of
 luminance images, 2) the position of the fovea, and 3) parameters of the
 cost function (described in the next section).
 The latter would normally be static throughout a task, but this is not
 a requirement.
 The outputs after processing each frame are 1) a disparity map, and 2)
 the position of the fovea for the next frame.
\end_layout

\begin_layout Standard
Downsampled copies of the disparity estimate 
\begin_inset Formula $D_{k}$
\end_inset

 are stored for several frames to allow integration of information over
 time.
 Frames 
\begin_inset Formula $D_{k-n},...,D_{k}$
\end_inset

 are used to produce estimates of both disparity and its uncertainty for
 frame 
\begin_inset Formula $k$
\end_inset

.
 We sometimes retained only the foveal regions of past frames rather than
 full images.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename system-sketch.pdf
	scale 60
	BoundingBox 130bp 150bp 590bp 390bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
General structure of our approach.
 The 2D boxes correspond to single-channel luminance and disparity images.
 The thicker boxes correspond to image variables with more channels, including
 the data cost over multiple disparities (~100 channel images) and disparity
 history over multiple frames (~5 channels).
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Remapping Past Frames
\end_layout

\begin_layout Standard
As mentioned in the Introduction, the fovea was placed in each frame so
 that it covered the part of the image that seemed to be the most important
 for the task, specifically the part in which a preliminary estimate of
 disparity most greatly exceeded the time-averaged disparity per pixel.
 The preliminary disparity estimate in frame 
\begin_inset Formula $k$
\end_inset

 was obtained by mapping the estimate 
\begin_inset Formula $d_{k-1}^{*}$
\end_inset

 from the previous frame into the coordinates of the new frame.
 This mapping consisted of converting frame 
\begin_inset Formula $k-1$
\end_inset

 disparties to depth, then to 3D position in the left camera's coordinate
 system in frame 
\begin_inset Formula $k-1$
\end_inset

, then 3D position in the left camera's coordinate system in frame 
\begin_inset Formula $k$
\end_inset

, and finally to depth and disparity from the frame-
\begin_inset Formula $k$
\end_inset

 perspective.
 
\end_layout

\begin_layout Standard
Due to changes in perspective, there was not a one-to-one map between pixels
 in the frame 
\begin_inset Formula $k-1$
\end_inset

 and frame 
\begin_inset Formula $k$
\end_inset

 disparity images, so the remapped disparity images contained blank pixels.
 We experimented with two ways of filling in the blank pixels.
 The first was linear interpolation, and the second was setting blank pixels
 to zero and taking the maximum over small neighbourhoods.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:interp"

\end_inset

 compares these two methods.
 Taking the maximum was about XX times faster than bilinear interpolation,
 with comparable results, so we used the maximum.
 
\end_layout

\begin_layout Standard
Some pixels also corresponded to multiple previous disparities per frame
 rather than zero or one.
 For these pixels, the largest disparity was chosen, corresponding to the
 nearest surface.
 [TODO: is this true?] 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/smudge-vs-interp.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of interpolation methods for remapped data.
 The top left panel is a disparity map 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

.
 The top right is the previous frame's estimate, 
\begin_inset Formula $d_{x,y,k-1}^{*}$
\end_inset

, remapped into the current frame's perspective.
 Aside from the missing data (dark blue points) this is a fair approximation
 of the current disparity estimate, because in this frame nothing in the
 scene is moving except the cameras.
 The bottom right panel shows linear interpolation of the remapped data.
 The bottom left panel is the maximum of the remapped image and copies of
 this image shifted one pixel vertically and one pixel both left and right.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:interp"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Using Information from Past Frames in Disparity Estimates
\end_layout

\begin_layout Standard
Disparity has temporal as well as spatial correlations, and further improvements
 in accuracy beyond spatial BP can be obtained by also propagating information
 over time (Zhu et al., 2010).
 However, this process is expensive in terms of runtime and memory.
 We sought to inexpensively approximate this approach by adding the cost
 term,
\begin_inset Formula 
\[
S_{x,y,d,k}=\min\left\{ \sum_{i-1}^{n}\left|d-P_{k}(d_{x,y,k-i}^{*})\right|,S_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $P_{k}$
\end_inset

 indicates mapping into the frame-
\begin_inset Formula $k$
\end_inset

 perspective and 
\begin_inset Formula $S_{max}$
\end_inset

 is the level at which this cost term saturates.
 Saturation prevents excessive penalization of legitimate large differences
 between frames, for example when something in the environment moves.
 This approach has modest runtime and storage costs, for example messages
 from previous frames need not be stored.
 To further reduce the cost of storing and remapping past disparities, we
 stored only the foveal regions of previous disparity images (as these were
 expected to be the most accurate), and we downsampled them.
 
\end_layout

\begin_layout Standard
This method also allows the algorithm to ignore information from past frames
 where the disparity is unambiguous in the current frame.
 This is important, because predictions from past frames can be less accurate
 than current estimates, e.g.
 due to extrinsic motion in the scene.
 Other inexpensive ways of using information from past frames, such as low-pass
 filtering the depth, do not have this advantage.
 
\end_layout

\begin_layout Subsection
Fovea Placement
\end_layout

\begin_layout Standard
[TODO: describe a hypothetical alternative importance calculation for grasping
 based on Platt's method with low-res stereo first (discussion)]
\end_layout

\begin_layout Standard
Fovea placement was task-dependent.
 In general, it was intended to minimize task-specific cost functions of
 the form, 
\begin_inset Formula 
\[
C_{x,y}=w_{x,y}(d_{x,y}^{*}-d_{x,y})^{2},
\]

\end_inset

where 
\begin_inset Formula $d_{x,y}$
\end_inset

 is ground-truth disparity and 
\begin_inset Formula $w_{x,y}$
\end_inset

 is a weight map that has large values in the most task-relevant regions.
 Weights 
\begin_inset Formula $w_{x,y}$
\end_inset

 depended on the task and also on a preliminary disparity estimate 
\begin_inset Formula $d_{x,y,k}^{0}=P_{k}(d_{x,y,k-1}^{*})$
\end_inset

.
 
\end_layout

\begin_layout Standard
One possible weighting scheme for navigation would be to emphasize regions
 in the direction of travel, because the risk of collisions with obstacles
 is greatest in this direction.
 Another would be to emphasize regions in which surfaces are relatively
 close (i.e.
 disparity is high).
 However, both of these schemes emphasize the close parts of the ground,
 which is often clear of obstacles.
 Instead, we defined 
\begin_inset Formula 
\[
w_{x,y}^{d}=\max(d_{x,y}^{0}-\bar{d}_{x,y},0),
\]

\end_inset

where 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 is the mean of 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 over 
\begin_inset Formula $k$
\end_inset

.
 This approach emphasizes parts of the image in which surfaces are closer
 than usual.
 We used 
\begin_inset Formula $w_{x,y}=w_{x,y}^{d}w_{x,y}^{s}$
\end_inset

, where 
\begin_inset Formula $w_{x,y}^{s}$
\end_inset

 is a static weight template that is highest in the horizontal centre of
 the image (the direction of travel) and also lower at the top of the image
 than the bottom.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:weighting-example"

\end_inset

 shows an example frame from the KITTI dataset (XX) in which a cyclist (a
 nearby obstacle) is strongly emphasized by this method.
 
\end_layout

\begin_layout Standard
For simplicity, we calculated 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 over full videos before processing, which is unrealistic for deployment
 on a robot.
 However, a recursive low-pass filter with a long time constant would have
 a similar effect with small computational cost.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/importance.png
	scale 50

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:weighting-example"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Weighting by disparity in excess of the average.
 Top: Average disparity 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 estimated over a long image sequence.
 Center: Disparity 
\begin_inset Formula $d_{x,y}^{*}$
\end_inset

 estimated in a single frame.
 Bottom: The weight 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

, where 
\begin_inset Formula $w^{s}$
\end_inset

 is a static weight template that emphasizes the direction of travel, and
 
\begin_inset Formula $w_{x,y}^{d}=\max(d_{x,y}^{*}-\bar{d}_{x,y},0)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The fovea was positioned so that it covered the region with the highest
 total 
\begin_inset Formula $w_{x,y}$
\end_inset

, calculated efficiently using an integral image.
 This approach minimizes the cost if the squared error 
\begin_inset Formula $(d_{x,y}^{*}-d_{x,y})^{2}$
\end_inset

 is statistically uniform over the image, and lower within than outside
 the fovea.
 We note that it would also be possible to account for non-uniform squared
 error, for example by weighing pixels with broader cost functions over
 disparity more heavily for fovea placement.
 
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
[TODO: clean this up.] The methods were tested on data from the KITTI Vision
 Benchmark Suite [TODO: ref].
 This dataset includes high-resolution rectified stereo images (1242x375
 pixels) taken from the roof of a car during city driving.
 In addition to isolated stereo images, the dataset includes a number of
 video sequences, supporting methods (such as ours) which propagate information
 over time.
 Furthermore, ground-truth depth is available from a LIDAR sensor for parts
 of the image.
 [TODO: how do we deal with time offsets] The scenes include static obstacles
 such as buildings and parked cars, and moving obstacles such as cars, pedestria
ns, and people on bicycles.
 
\end_layout

\begin_layout Subsection
Parameters
\end_layout

\begin_layout Standard
[TODO: clean this up.] The system parameters and their values for different
 tests are listed in Table
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:parameters"

\end_inset

.
 Optimal parameters were found using the package Hyperopt 
\begin_inset CommandInset citation
LatexCommand cite
key "Bergstra2013"

\end_inset

.
 Hyperopt attempts to remove the irreproducible ``art'' of hand-tuning hyperpara
meters, instead using an automated method for searching hyperparameter space
 to find optimal (or near-optimal) values.
 [TODO: hyperopt procedure on full BP for ground truth and on our method]
\end_layout

\begin_layout Standard
Table XX lists the parameters that we used for our performance tests (see
 section XX).
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
System parameters and values used in performance tests.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of disparities
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Area of fovea
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
X x X pixels
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Frame history length
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BP iterations per scale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data cost saturation value (
\begin_inset Formula $C_{max}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Discontintuity cost saturation value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Peripheral # scales
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Foveal # scales
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
History downsampling factor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:parameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

 shows two disparity estimates from a single frame, with the fovea in different
 places (the centres are marked with white + signs).
 These examples were processed with multiple levels of resolution, with
 a small high-resolution region in the centre and multiple levels of decreasing
 resolution with greater distance from the centre, analogous to the human
 eye and visual cortex.
 The total runtime of BP in these examples is only a few times as great
 as BP at the lowest resolution.
 In the top frame, the fovea clearly resolves the cyclist, while in the
 bottom frame the fovea correctly interprets a low-disparity region in which
 the top frame has an artefact.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-examples.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of foveal multiscale belief propagation.
 The white + marks indicate fovea centres.
 These examples were processed at multiple resolutions, with equal computational
 demands at each of these resolutions (e.g.
 a parafoveal region twice the size of the fovea had half the resolution).
 In the top panel, the fovea resolves a cyclist at high resolution.
 There is a large artefact in the top-centre of the image.
 In the bottom panel (same frame), the fovea is instead centred on this
 artefact, and corrects it.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:fovea-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:seed-example"

\end_inset

 shows an example of the disparity estimation that is improved through the
 use of information from a previous frame.
 The top panel is the same as that of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

.
 It contains an artefact in the sky, which is featureless in this scene
 and therfore has ambiguous disparity.
 The centre panel is a remapped disparity estimate from a foveal and parafoveal
 region in the previous frame, overlapping with the artefact in the top
 centre.
 The bottom panel shows a new estimate (with the fovea on the cyclist, similar
 to that of the top panel) but with an additional cost term that penalizes
 deviations from the remapped estimate in the centre panel.
 This example illustrates that information from previous frames has the
 potential to improve disparity estimates, as one would expect due to correlatio
ns over time.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/seed-vs-no-seed.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of the use of information from a previous frame to improve disparity
 estimation.
 Top: Estimate with fovea on cyclist, as the top panel of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

.
 Centre: A remapped estimate from the fovea of the previous frame, in which
 the fovea was in the top centre of the image.
 Bottom: New disparity estimate with fovea on the cyclist, but also using
 the prior estimate from the centre panel.
 The prior estimate resolves the ambiguity in the current frame, and corrects
 the artefact.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:seed-example"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
- [TODO: flops per frame]
\end_layout

\begin_layout Standard
- [TODO: examples and averages of least-squared error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: examples of asymmetric error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: examples of asymmetric weighted error on KITTI data]
\end_layout

\begin_layout Standard
- [TODO: example for grasping with another dataset]
\end_layout

\begin_layout Standard
- [TODO: frame rate]
\end_layout

\begin_layout Standard
- [TODO: error vs.
 somewhat less coarse at same frame rate]
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
[TODO: summarize main results]
\end_layout

\begin_layout Standard
[TODO: discuss additional computation and memory needed vs.
 BP computation]
\end_layout

\begin_layout Standard
[TODO: emphasize that this would probably work with other stereo methods
 too, maybe even local ones at low/high resolution on low-end hardware (although
 would the rest of the system dominate runtime?)]
\end_layout

\begin_layout Standard
Our system focuses resources on different parts of the scene by moving an
 analysis window.
 An alternative approach is to move the cameras (refs).
 This approach might be particularly useful with yoked foveal and peripheral
 cameras that have different fields of view (the foveal one having a narrower
 field of view and finer detail).
 However, this approach would involve further cost and complexity, inlcuding
 four actuators to move each camera assembly in two degrees of freedom.
 Actuation at the speeds required to move the fovea frequently is not straightfo
rward.
 In human saccades, rotational acceleration can exceed 
\begin_inset Formula $20,000{}^{\circ}/s^{2}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Abrams1989"

\end_inset

.
 A recent system that achieved appropriate speeds (ref Villgrattner) used
 actuators that cost thousands of dollars each.
 
\end_layout

\begin_layout Subsection
Future Work
\end_layout

\begin_layout Standard
This approach is suited to navigation in mobile robots with limited processing
 power.
 We are specifically intersted in adapting this system for a panoramic catadoptr
ic stereo sensor that is in development, and testing it in navigation among
 pedestrians.
 This may allow further fine-tuning of the importance weights.
 For example the weights could be more directly tied to decisions that the
 robot must make, e.g.
 whether a narrow gap is too narrow to navigate.
 
\end_layout

\begin_layout Standard
Another potential direction for future work would be to adapt the method
 to parallel hardware such as a GPU or FPGA.
 The multiple-resolution version of our approach (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

) is well suited for such an implementation, because each scale has the
 same number of pixels.
 For example, if the fovea is 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels, then the immediate parafoveal region is 
\begin_inset Formula $2m$
\end_inset

 by 
\begin_inset Formula $2n$
\end_inset

, downsampled by a factor of two, for a total of 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels.
 An FPGA with a capacity for 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

 pixels could process a frame in a few steps, potentially yielding very
 high framerates with modest hardware.
 
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Section*
Acknowledgements 
\end_layout

\begin_layout Standard
This work was supported in part by a Mitacs and CrossWing Inc.
 The authors thank John-Paul Gignac of CrossWing Inc.
 for helpful discussions.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "fast-stereo"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
