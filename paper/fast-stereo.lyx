#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url}
%\usepackage{natbib}
\usepackage[authoryear]{natbib}
\DeclareMathOperator*{\argmin}{arg\,min}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Task-Driven Resource Allocation for Real-Time Stereo with Belief Propagation
\end_layout

\begin_layout Author
Eric Hunsberger, Jeff Orchard & Bryan Tripp
\end_layout

\begin_layout Abstract
Stereo matching is a versatile approach to depth estimation.
 However, accurate algorithms for dense stereo do not run at frame rates
 that are practical for robotics.
 In this paper we explore an approach that focuses computational resources
 for depth estimation on important parts of the scene.
 Specifically, we use multiscale loopy belief propagation, but only run
 the finest scales in a small selected region that we call the 
\begin_inset Quotes eld
\end_inset

fovea
\begin_inset Quotes erd
\end_inset

 (as its purpose is analogous to the high-resolution region of the human
 retina).
 The fovea is moved in each frame to minimize a task-dependent cost function.
 Its movement is therefore determined by a combination of task parameters
 and image features.
 We evaluate this approach on stereo videos from the KITTI Vision Benchmark
 Suite, using a simple cost function that emphasizes pixels with higher-than-nor
mal disparity for their image location.
 We show that this approach can improve depth estimates within a real-time
 performance constraint.
 We discuss analogies with task-dependent control of human eye movements.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Information about the locations of surfaces in the environment is essential
 for robot navigation, manipulation, etc.
 Stereoscopic disparity (i.e.
 difference in the location of a given feature in images from two offset
 cameras) is a rich source of this information.
 Disparity is inversely related to the feature's distance (depth) from the
 cameras.
 Compared to other approaches to depth estimation, stereo is currently more
 cost effective than LIDAR, and it can provide estimates from a large field
 of view at high frame rates.
 It is also more economical and higher-resolution than time-of-flight devices.
 In contrast with systems that rely on patterned infrared illumination (e.g.
 the Microsoft Kinect), it is not limited by infrared interference from
 sunlight.
 
\end_layout

\begin_layout Standard
The first step in stereo depth estimation is to match image features in
 one camera with those in the other camera.
 This is done by comparing a small region of one image with a range of horizonta
lly-offset regions in the other image.
 The most similar region in the second image (or perhaps one of the few
 most similar regions) is relatively likely to correspond to the same point
 in physical space.
 
\emph on
Sparse
\emph default
 stereo methods match only a minority of features, i.e.
 distinctive ones that are likely to be matched correctly 
\begin_inset CommandInset citation
LatexCommand cite
key "Dhond1989"

\end_inset

.
 In contrast, 
\emph on
dense
\emph default
 methods provide estimates over the whole field of view, even in featureless
 regions.
 Dense methods are of interest in robotics, because surface locations may
 affect the robot whether or not they are filled with distinctive visual
 features.
 However, in the absence of distinctive features (e.g.
 in an image of a textureless wall), a wide range of regions may match equally
 well.
\end_layout

\begin_layout Standard
Neighbouring disparities are highly correlated except at object boundaries.
 If an ambiguous region (with a broad probability density function spread
 over a wide range of disparities) is close to unambiguous regions (with
 narrow probability densities), then accounting for spatial correlations
 can profoundly improve estimates in the ambiguous region.
 An effective approach is to model the disparity map as a Markov random
 field.
 Then, maximum 
\emph on
a posteriori
\emph default
 depth images can be estimated using various methods, such as loopy belief
 propagation and graph cuts 
\begin_inset CommandInset citation
LatexCommand cite
key "Tappen2003,Szeliski2008"

\end_inset

.
 But despite many recent advances in efficiency, these methods remain computatio
nally intensive, limiting their practicality for robots, which require high
 frame rates with modest computational resources.
 
\end_layout

\begin_layout Standard
In this study we explore the benefits of processing different parts of the
 scene in different levels of detail.
 This general strategy is based on the primate visual systems.
 Primates use disparity as a depth cue, but in contrast with conventional
 image processing, a disproportionately large amount of cortex is dedicated
 to the centre of the visual field (the fovea), with progressively fewer
 cortical resources available for more peripheral parts of the visual field
 
\begin_inset CommandInset citation
LatexCommand cite
key "Daniel1961"

\end_inset

.
 The eyes move frequently, typically jumping to a new target several times
 per second, to sense and analyze detail from different parts of the scene
 in series.
 Analogously, in this study we process 
\begin_inset Quotes eld
\end_inset

foveal
\begin_inset Quotes erd
\end_inset

 image regions at high resolution and 
\begin_inset Quotes eld
\end_inset

peripheral
\begin_inset Quotes erd
\end_inset

 regions at lower resolution in order to save computation time.
 
\end_layout

\begin_layout Standard
The key problem is where to position the fovea in each frame.
 Primates address this problem through sophisticated systems for directing
 visual attention and eye movements (reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Kowler2011,Hayhoe2005,Schutz2011"

\end_inset

).
 Computational models of these systems, and applications to computer vision
 and robotics, are reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Borji2013,Frintrop2010,Kimura2013"

\end_inset

[TODO: also Tsotsos (2011)].
 As discussed in these reviews, human eye movements are occasionally directed
 to salient visual features (such as the onset of motion) but in most situations
 are overwhelmingly determined by task demands.
 Examples of task-dependent targets include the next word while reading
 
\begin_inset CommandInset citation
LatexCommand cite
key "Rayner2010"

\end_inset

, the edge of an obstacle while navigating 
\begin_inset CommandInset citation
LatexCommand cite
key "Rothkopf2007"

\end_inset

, an object a person wants to pick up 
\begin_inset CommandInset citation
LatexCommand cite
key "Johansson2001"

\end_inset

, etc.
 The visual target can even be a completely featureless region.
 For example people often glance at a spot where they intend to put something,
 even though the spot may be visually indistinct 
\begin_inset CommandInset citation
LatexCommand cite
key "Tatler2011"

\end_inset

.
 However, eye movements are very often determined by a combination of bottom-up
 and top-down factors.
 
\end_layout

\begin_layout Standard
A simple example of bottom-up / top-down interaction occurs in visual search
 tasks.
 Viewing an image of many small shapes, humans can rapidly find shapes with
 a distinctive feature (e.g.
 the yellow ones; the horizontal ones; etc.) Interestingly, visual search
 for more complex conjunctions of features (e.g.
 horizontal yellow shapes) is slower and less automatic 
\begin_inset CommandInset citation
LatexCommand cite
key "Treisman1980"

\end_inset

.
\end_layout

\begin_layout Standard
Taking inspiration from the primate visual system, our goal here is to obtain
 practical real-time stereo depth estimates by allocating computational
 resources to the most task-relevant image regions in each frame.
 Taking further inspiration from the visual search literature, we direct
 the fovea using a simple visual feature that is relevant to visual navigation.
 Specifically, we use previous frames to estimate an expected diparity map,
 and define an 
\begin_inset Quotes eld
\end_inset

importance map
\begin_inset Quotes erd
\end_inset

 as the half-rectified difference between expected disparity and a time-average
 disparity (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Maki2000"

\end_inset

 for another use of depth as an attention cue).
 This allows us to direct the fovea to regions in which surfaces seem to
 be closer than normal for their part of the visual field, which often correspon
d to obstacles.
 This approach leads to improved estimation of obstacle surfaces at practical
 frame rates.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
Our general strategy is to perform fast disparity estimation over most of
 the image, and more accurate estimation in a small window, and to move
 this window each frame to minimize a task-specific cost function.
 This general strategy could be used with a variety of stereo algorithms,
 but we focus here on a specific implementation of the strategy that uses
 loopy belief propagation.
 
\end_layout

\begin_layout Subsection
Markov Random Fields and Loopy Belief Propagation
\end_layout

\begin_layout Standard
Pixel-by-pixel stereo correspondances are frequently ambiguous, so disparity
 estimation is improved by considering spatial correlations.
 The full correlation matrix is unmanageable, becaused disparity images
 of practical interest have tens of thousands of pixels or more.
 A successful approach 
\begin_inset CommandInset citation
LatexCommand cite
key "Sun2003"

\end_inset

 has been to model disparity images as Markov random fields.
 In a Markov random field (MRF), each region is independent of the rest
 of the field, conditional on values in the region's boundary.
 That is, 
\begin_inset Formula $p(x_{i}|x_{b},x_{o})=p(x_{i}|x_{b})$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is the part of the field inside the boundary, 
\begin_inset Formula $x_{b}$
\end_inset

 is the part that comprises the boundary, and 
\begin_inset Formula $x_{o}$
\end_inset

 is the part outside the boundary 
\begin_inset CommandInset citation
LatexCommand cite
key "Fieguth2010"

\end_inset

.
 There are various ways to estimate the maximum 
\emph on
a posteriori
\emph default
 disparity from the MRF.
 
\end_layout

\begin_layout Standard
One such method is belief propagation (BP), a general algorithm for inference
 on graphical models (inlcuding Bayesian networks as well as MRFs).
 BP provides exact solutions on trees.
 Loops in the statistical relationships of Markov random fields prevent
 exact inference, but 
\begin_inset Quotes eld
\end_inset

loopy
\begin_inset Quotes erd
\end_inset

 BP typically produces good approximations after running for a few iterations
 
\begin_inset CommandInset citation
LatexCommand cite
key "Murphy1999"

\end_inset

.
 
\end_layout

\begin_layout Standard
The starting point for our work is a BP implementation 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 that has several optimizations, which together accelerate the algorithm
 by several orders of magnitude.
 One of these optimizations is a multiscale method that reduces the numbers
 of iterations needed to propagate information to distant parts of the image.
 Our modification consists simply of executing finer scales (which take
 up most of the computation time) only in sub-images rather than over the
 whole image.
 
\end_layout

\begin_layout Standard
The BP implementation of 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 results in a labelling 
\begin_inset Formula $f$
\end_inset

, which assigns a label 
\begin_inset Formula $f_{p}\in\mathcal{L}$
\end_inset

 to pixel 
\begin_inset Formula $p$
\end_inset

, where 
\begin_inset Formula $\mathcal{L}$
\end_inset

 consists of all possible disparities.
 The algorithm minimizes an energy function 
\begin_inset Formula 
\[
E(f)=\sum_{p\in\mathcal{P}}D_{p}(f_{p})+\sum_{p,q\in\mathcal{N}}V(f_{p}-f_{q})
\]

\end_inset

where 
\begin_inset Formula $D_{p}$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

data cost
\begin_inset Quotes erd
\end_inset

) is the cost of labelling pixel 
\begin_inset Formula $p$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

, and 
\begin_inset Formula $V(f_{p}-f_{q})$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

discontinuity cost
\begin_inset Quotes erd
\end_inset

) is an additional cost of labelling neighbouring pixels 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

 and 
\begin_inset Formula $f_{q}$
\end_inset

.
 Because the disparity of neighbouring pixels is strongly correlated, a
 larger discontinuity cost is assigned for larger differences between neighbours.
 Minimizing 
\begin_inset Formula $E(f)$
\end_inset

 corresponds to maximum 
\emph on
a posteriori
\emph default
 estimation in a probabilistic context 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

.
 
\end_layout

\begin_layout Standard
BP involves iterative computation and exhange of messages between pixels.
 In iteration 
\begin_inset Formula $t$
\end_inset

, the message 
\begin_inset Formula $m_{p\to q}^{t}$
\end_inset

 from pixel 
\begin_inset Formula $p$
\end_inset

 to pixel 
\begin_inset Formula $q$
\end_inset

 is 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset


\begin_inset Formula 
\[
m_{p\to q}^{t}=min_{f_{p}}\left(V(f_{p}-f_{q})+D_{p}(f_{p})+\sum_{s\in\mathcal{N}(p)\backslash q}m_{s\to p}^{t-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\mathcal{N}(p)\backslash q$
\end_inset

 consists of the neighbours of 
\begin_inset Formula $p$
\end_inset

 other than 
\begin_inset Formula $q$
\end_inset

.
 After 
\begin_inset Formula $T$
\end_inset

 iterations, final labels 
\begin_inset Formula $f_{p}^{*}$
\end_inset

 are assigned as 
\begin_inset Formula 
\[
f_{p}^{*}=\argmin_{f_{p}}D(f_{p})+\sum_{q\in\mathcal{N}(p)}m_{q\to p}^{T}(f_{p}).
\]

\end_inset

 
\end_layout

\begin_layout Standard
In stereo estimation, the first step in this process is to calculate a data
 cost volume in terms of image coordinates 
\begin_inset Formula $p=x,y$
\end_inset

 (where 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are the horizontal and vertical image coordinates, respectively), and disparity
 (in pixels) 
\begin_inset Formula $d$
\end_inset

.
 The data cost volume is specifically 
\begin_inset Formula 
\[
D_{x,y,d}=\min\left\{ \left|I_{x,y}^{l}-I_{x-d,y}^{r}\right|,D_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $I^{l}$
\end_inset

 and 
\begin_inset Formula $I^{r}$
\end_inset

 are luminance of the left and right images, and 
\begin_inset Formula $D_{\mathrm{max}}$
\end_inset

 is a saturation value that limits the cost of large discontinuities.
 The saturation value is used because while most discontinuities are expected
 to be small, some (at object boundaries) are expected to be larger, with
 no particular expectation about how much larger.
 
\end_layout

\begin_layout Standard
Prior to calculating the data cost, we processed the images with a Laplacian
 filter, in order to emphasize edges.
 
\end_layout

\begin_layout Standard
We refer to the disparity estimated from BP in frame 
\begin_inset Formula $k$
\end_inset

 as 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 .
\end_layout

\begin_layout Subsection
System Overview
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:motivation"

\end_inset

 illustrates the motivation for our approach.
 The two curves show error in stereo disparity estimation with different
 image resolutions.
 The lower-resolution images (left curve) are downsampled by a factor of
 two in each dimension, relative to the higher-resolution images.
 Downsampling decreases runtime and increases error.
 For a given resolution, error decreases with increasing numbers of iterations
 of belief propagation (BP), but most of this decrease occurs in the first
 few iterations.
 Beyond 5-10 iterations, error can only be reduced substantially further
 by switching to a higher resolution (e.g.
 by switching from the left curve to the right curve).
 
\end_layout

\begin_layout Standard
Suppose we have a certain time budget per frame (e.g.
 1/4s) and the right curve is entirely outside it.
 In this case, we can only afford to process the full frame at lower resolution.
 However, it may be possible to reduce error somewhat below the left curve,
 by processing only part of each frame at higher resolution.
 This part can be whatever size uses all the available time.
 Furthermore, in many applications (e.g.
 navigation, grasping), some areas in each frame are likely to be more important
 than others.
 If the most important areas are processed at higher resolution, then the
 runtime may remain acceptable while the importance-weighted error approaches
 that of higher-resolution BP.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-rationale.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Performance of multiscale belief propagation on example data, illustrating
 the motivation for our approach.
 Disparity estimation error is plotted vs.
 runtime.
 The right curve (circles) were obtained with a high-resolution stereo image
 pair, and the left curve (squares; higher error) with lower-resoluton versions
 of the same images.
 Within each curve, the runtime varies with numbers of iterations at each
 scale (1, 2, 3, 4, 5, 7, 10, and 15 iterations).
 Given a time budget of, for example, 0.25s/frame, it would not be possible
 to process these images at high resolution.
 However, if certain areas in the images were of greater practical interest,
 then results that are nearly as useful might be achieved by processing
 just those areas at high resolution.
 The data are 25 frames taken from the KITTI dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

, downsampled by a factor of two (high resolution) and four (low resolution)
 in each dimension.
 [TODO: rerun with ground truth and MAD cost]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:motivation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The system performs multiscale belief propagation (BP) with different numbers
 of scales in different parts of the image.
 The number of operations in a single scale of Felzenszwalb & Huttenlocher's
 method is order 
\begin_inset Formula $O(nl)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of pixels and 
\begin_inset Formula $l$
\end_inset

 is the number of disparities.
 The majority of the computational cost is incurred at the finest scale,
 which has four times as many pixels as the next-finest scale.
 Our system only processes the finest one or two scales in the foveal region.
 The resulting depth estimate is similar to that of full multiscale BP in
 the fovea, with nearly the same speedup one would get by omitting the finest
 scale(s).
 
\end_layout

\begin_layout Standard
Importantly, messages from coarser scales are used to initialize messages
 in fine foveal scales.
 This allows information from coarse scales to propagate from other image
 regions across the fovea, makes depth estimates in the fovea continuous
 with those in the surrounding areas, and also makes them similar to those
 of full belief propagation.
 They differ somewhat around the fovea border, due to propagation across
 the border in the fine scale of full BP.
 
\end_layout

\begin_layout Standard
Figure 1 illustrates our general approach.
 The inputs to the system in each frame are 1) a rectified stereo pair of
 luminance images, 2) the position of the fovea, and 3) parameters of the
 cost function (described in the next section).
 The latter would normally be static throughout a task, although this is
 not essential.
 The outputs after processing each frame are 1) a disparity map, and 2)
 the position of the fovea for the next frame.
\end_layout

\begin_layout Standard
Downsampled copies of the disparity estimate 
\begin_inset Formula $D_{k}$
\end_inset

 are stored for several frames to allow integration of information over
 time.
 Frames 
\begin_inset Formula $D_{k-n},...,D_{k}$
\end_inset

 are used to produce estimates of both disparity and its uncertainty for
 frame 
\begin_inset Formula $k$
\end_inset

.
 We sometimes retained only the foveal regions of past frames rather than
 full images.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename system-sketch.pdf
	scale 60
	BoundingBox 130bp 150bp 590bp 390bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
General structure of our approach.
 The 2D boxes correspond to single-channel luminance and disparity images.
 The thicker boxes correspond to image variables with more channels, including
 the data cost over multiple disparities (~100 channel images) and disparity
 history over multiple frames (~5 channels).
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Remapping Past Frames
\end_layout

\begin_layout Standard
As mentioned in the Introduction, the fovea was placed in each frame so
 that it covered the part of the image that seemed to be the most important
 for the task, specifically the part in which a preliminary estimate of
 disparity most greatly exceeded the time-averaged disparity per pixel.
 If odometry is available, the preliminary estimate in frame 
\begin_inset Formula $k$
\end_inset

 can be obtained by mapping the estimate 
\begin_inset Formula $d_{k-1}^{*}$
\end_inset

 from the previous frame into the coordinates of the new frame.
 This mapping consists of converting frame 
\begin_inset Formula $k-1$
\end_inset

 disparties to depth, then to 3D position in the left camera's coordinate
 system in frame 
\begin_inset Formula $k-1$
\end_inset

, then 3D position in the left camera's coordinate system in frame 
\begin_inset Formula $k$
\end_inset

, and finally to depth and disparity from the frame-
\begin_inset Formula $k$
\end_inset

 perspective.
 
\end_layout

\begin_layout Standard
Due to changes in perspective, there is not a one-to-one map between pixels
 in the frame 
\begin_inset Formula $k-1$
\end_inset

 and frame 
\begin_inset Formula $k$
\end_inset

 disparity images, so remapped disparity images contain blank pixels.
 We experimented with two ways of filling in the blank pixels.
 The first was linear interpolation, and the second was setting blank pixels
 to zero and taking the maximum over small neighbourhoods.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:interp"

\end_inset

 compares these two methods.
 Taking the maximum was much faster than bilinear interpolation, with comparable
 results, so we used the maximum.
 
\end_layout

\begin_layout Standard
Some pixels also corresponded to multiple previous disparities per frame
 rather than zero or one.
 For these pixels, the largest disparity was chosen, corresponding to the
 nearest surface.
 [TODO: is this true?]
\end_layout

\begin_layout Standard
We also experimented with using remapped disparity estimates from previous
 frames to improve current-frame estimates.
 Specifically, we augmented the data cost to penalize changes over time.
 In some cases this method removed artefacts.
 However, we were unable to clearly evaluate whether this improved performance
 on average, because the KITTI dataset does not provide odometry and ground
 truth for the same image sequences.
 Therefore we simply note that this is another potential use for remapped
 disparity.
 Relatedly, belief propagation can also be performed over time as well as
 image dimensions 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhu2010"

\end_inset

 although this is more computationally intensive.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/smudge-vs-interp.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of interpolation methods for remapped data.
 The top left panel is a disparity map 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

.
 The top right is the previous frame's estimate, 
\begin_inset Formula $d_{x,y,k-1}^{*}$
\end_inset

, remapped into the current frame's perspective.
 Aside from the missing data (dark blue points) this is a fair approximation
 of the current disparity estimate, because in this frame nothing in the
 scene is moving except the cameras.
 The bottom right panel shows linear interpolation of the remapped data.
 The bottom left panel is the maximum of the remapped image and copies of
 this image shifted one pixel vertically and one pixel both left and right.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:interp"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Fovea Placement
\end_layout

\begin_layout Standard
Fovea placement was task-dependent.
 In general, it was intended to minimize task-specific cost functions of
 the form, 
\begin_inset Formula 
\[
C_{x,y}=w_{x,y}(d_{x,y}^{*}-d_{x,y})^{2},
\]

\end_inset

where 
\begin_inset Formula $d_{x,y}$
\end_inset

 is ground-truth disparity and 
\begin_inset Formula $w_{x,y}$
\end_inset

 is a weight map that has large values in the most task-relevant regions.
 Weights 
\begin_inset Formula $w_{x,y}$
\end_inset

 depended on the task and also on a preliminary disparity estimate 
\begin_inset Formula $d_{x,y,k}^{0}$
\end_inset

, which was either obtained by remapping past frames, when odometry was
 available, or by running BP only at coarse scales.
 
\end_layout

\begin_layout Standard
One possible weighting scheme for navigation would be to emphasize regions
 in the direction of travel, because the risk of collisions with obstacles
 is greatest in this direction.
 Another would be to emphasize regions in which surfaces are relatively
 close (i.e.
 disparity is high).
 However, both of these schemes emphasize the close parts of the ground,
 which is often clear of obstacles.
 Instead, we defined 
\begin_inset Formula 
\[
w_{x,y}^{d}=\max(d_{x,y}^{0}-\bar{d}_{x,y}-d_{th},0),
\]

\end_inset

where 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 is the mean of 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 over 
\begin_inset Formula $k$
\end_inset

, and 
\begin_inset Formula $d_{th}$
\end_inset

 is a small threshold.
 This approach emphasizes parts of the image in which surfaces are closer
 than usual.
 We used 
\begin_inset Formula $w_{x,y}=w_{x,y}^{d}w_{x,y}^{s}$
\end_inset

, where 
\begin_inset Formula $w_{x,y}^{s}$
\end_inset

 is a static weight template that is highest in the horizontal centre of
 the image (the direction of travel) and also lower at the top of the image
 than the bottom.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:weighting-example"

\end_inset

 shows an example frame from the KITTI dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

 in which a cyclist (a nearby obstacle) is strongly emphasized by this method.
 
\end_layout

\begin_layout Standard
For simplicity, we calculated 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 over full videos before processing, which is unrealistic for deployment
 on a robot.
 However, a recursive low-pass filter with a long time constant would have
 a similar effect with small computational cost.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/importance-tight.png
	scale 50

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:weighting-example"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Weighting by disparity in excess of the average.
 Top: Average disparity 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 estimated over a long image sequence.
 Center: Disparity 
\begin_inset Formula $d_{x,y}^{*}$
\end_inset

 estimated in a single frame.
 Bottom: The weight 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

, where 
\begin_inset Formula $w^{s}$
\end_inset

 is a static weight template that emphasizes the direction of travel, and
 
\begin_inset Formula $w_{x,y}^{d}=\max(d_{x,y}^{*}-\bar{d}_{x,y},0)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The fovea was positioned so that it covered the region with the highest
 total 
\begin_inset Formula $w_{x,y}$
\end_inset

, calculated efficiently using an integral image.
 This approach minimizes the cost if the squared error 
\begin_inset Formula $(d_{x,y}^{*}-d_{x,y})^{2}$
\end_inset

 is statistically uniform over the image, and lower within than outside
 the fovea.
 We note that it would also be possible to account for non-uniform squared
 error, for example by weighing pixels with broader cost functions over
 disparity more heavily for fovea placement.
 
\end_layout

\begin_layout Standard
Contrary to the example of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:weighting-example"

\end_inset

, the high-weight pixels are not always in one small region.
 Therefore we also experimented with dividing the fovea into multiple sub-foveas
 with the same total number of pixels.
 Whereas optimal placement of a single fovea with given dimensions simply
 corresponds to finding the maximum in a two-dimensional image, it is not
 practical to find the global optimal placement of multiple foveas.
 Instead, we used a greedy approach in which we placed a single sub-fovea
 optimally, set the weight within it to zero, placed another single subfovea
 optimally, etc.
 We calculated the total remaining weight with different numbers of sub-foveas,
 and used the number with the lowest weight.
 These computations were all performed with images that were downsampled
 several times, so they were not computationally intensive.
 
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
The methods were tested on data from the 2012 KITTI Vision Benchmark Suite
 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

.
 This dataset includes high-resolution rectified stereo images (1242x375
 pixels) taken from the roof of a car during city driving, at ten frames/s.
 Rectified images and odometry are available for a number of long sequences.
 Ground truth depth (but not odometry) is available for single frames within
 a number of short 20-frame sequences.
 Ground truth is from a LIDAR sensor.
 The ground-truth points are fairly dense, because the authors combined
 points from several sweeps of the sensor using a semi-manual registration
 process.
 The scenes include static obstacles such as buildings and parked cars,
 and moving obstacles such as cars, pedestrians, and people on bicycles.
 
\end_layout

\begin_layout Subsection
Parameters
\end_layout

\begin_layout Standard
The system parameters and their values for different tests are listed in
 Table
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:parameters"

\end_inset

.
 Optimal parameters were found using the package Hyperopt 
\begin_inset CommandInset citation
LatexCommand cite
key "Bergstra2013"

\end_inset

.
 Hyperopt attempts to remove the irreproducible ``art'' of hand-tuning hyperpara
meters, instead using an automated method for searching hyperparameter space
 to find optimal (or near-optimal) values.
 [TODO: fill in parameters.] 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
System parameters and values used in performance tests.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Laplacian filter width
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data-cost weight
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data-cost saturation value (
\begin_inset Formula $C_{max}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Discontinuity cost saturation value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BP iterations per scale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XX
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:parameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

 shows two disparity estimates from a single frame, with the fovea in different
 places (the centres are marked with white + signs).
 These examples were processed with multiple levels of resolution, with
 a small high-resolution region in the centre and multiple levels of decreasing
 resolution with greater distance from the centre, analogous to the human
 eye and visual cortex.
 The total runtime of BP in these examples is only a few times as great
 as BP at the lowest resolution.
 In the top frame, the fovea clearly resolves the cyclist, while in the
 bottom frame the fovea correctly interprets a low-disparity region in which
 the top frame has an artefact.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-examples-tight.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of foveal multiscale belief propagation.
 The white + marks indicate fovea centres.
 These examples were processed at multiple resolutions, with equal computational
 demands at each of these resolutions (e.g.
 a parafoveal region twice the size of the fovea had half the resolution).
 In the top panel, the fovea resolves a cyclist at high resolution.
 There is a large artefact in the top-centre of the image.
 In the bottom panel (same frame), the fovea is instead centred on this
 artefact, and corrects it.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:fovea-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:foveation-sequence"

\end_inset

 shows a sequence of foveations while driving between parked cars.
 The sequence is from top to bottom.
 In this example a single fovea is used (i.e.
 without multiple sub-foveas).
 The cars on the right are foveated in turn.
 The cars on the left have slightly lower weights than those on the right,
 and are never foveated.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/foveation-sequence-tight.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example foveation sequence.
 [TODO: are there missing frames? seems like a large gap between last two]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:foveation-sequence"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:numbers-of-subfoveas"

\end_inset

 shows examples of sub-fovea placement, overlaid with weighting functions.
 In the top image the fovea has been broken into 30 sub-foveas that are
 placed individually.
 A greater number of smaller foveas can cover irregular shapes more closely.
 However, because our greedy method of sub-fovea placement is not globally
 optimal, more sub-foveas are not guaranteed to cover a larger cost.
 In this example, numbers of sub-foveas from 1-30 were compared (with the
 same total foveal pixels in each case), and two sub-foveas provided the
 best coverage (bottom image).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/multiple-foveas-30-vs-best.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of optimal placement of 30 sub-foveas (top) and 2 sub-foveas
 (bottom), with the same total numbers of pixels, overlaid with the correspondin
g weight image 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

.
 In the weight image, darker blue corresponds to low weight.
 In this example two sub-foveas provided the best coverage of weighted pixels
 of any number between 1 and 30.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:numbers-of-subfoveas"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Sub-Fovea Selection
\end_layout

\begin_layout Standard
The time taken by our greedy method of sub-fovea placement is roughly linear
 in the number of sub-foveas, and comparing all of 
\begin_inset Formula $\{1,2,..n\}$
\end_inset

 sub-foveas is quadratic.
 Although checking more possibilities will generally lead to better performance,
 it only makes sense to check possibilities that are relatively likely.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:num-subfoveas-histogram"

\end_inset

 shows a histogram of the numbers of sub-foveas with the best weight coverage
 over a set of example frames, up to a maximum of 30.
 Very often, two foveas were best.
 In further experiments we used the best of 1-5 sub-foveas.
 [TODO: repeat with more frames, various fovea areas]
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/best-num-foveas.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Frequency with which different numbers of foveas covered the greatest integrated
 weight in weight images 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

.
 This histogram is based on [TODO: image set] with [TODO: total pixels].
 Similar results were obtained with both greater and smaller total foveal
 areas.
 [TODO: test this] 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:num-subfoveas-histogram"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accuracy vs.
 Run Time
\end_layout

\begin_layout Standard
[TODO: see whether good peripheral performance disappears with non-pyramidal
 upsampling and downsampling]
\end_layout

\begin_layout Standard
We tested how both runtime and accuracy depended on the total foveal area.
 In general, our expectations were that runtime would increase almost linearly
 with the foveal area, and that accuracy would improve with increasing foveal
 area.
 For example, with zero foveal area, speed and accuracy should be nearly
 equal to that of full BP with down-sampled images.
 On the other hand, setting the fovea area equal to the image area should
 result in speed and accuracy equal to that of full multi-scale BP.
 We further anticipated that weighted error would drop sharply with increasing
 foveal area, since we tried to align the fovea with the highest-weight
 regions.
 
\end_layout

\begin_layout Standard
Figure [TODO: make this figure] shows an example of results that are in
 keeping with these expectations.
 The horizontal axis is the total foveal area as a fraction of image area.
 If we held the fovea at the centre of the image (XX line), weighted error
 dropped roughly linearly as a function of foveal area.
 In contrast, it dropped more steeply when a single fovea was aligned with
 high-weight regions, and more steeply still when several sub-foveas were
 allowed.
 
\end_layout

\begin_layout Standard
We found however that these results were sensitive to the way we calculated
 the data cost, and the way we upsampled coarse BP results in the periphery.
 In some configurations, errors were actually greater in the fovea than
 the periphery.
 In Figure XX, the data cost in the periphery was calculated once per pixel
 downsampled pixel (i.e.
 half the resolution of the fovea).
 Surprisingly superior results were obtained by calculating the data cost
 at each pixel (i.e.
 same resolution as the fovea), and downsampling by summing the results
 across groups of 2x2 pixels.
 
\end_layout

\begin_layout Standard
This counterintuitive outcome also relied on pyramidal upsampling [TODO:
 ref] of the peripheral disparity estimate.
 [TODO: check that is disappears without] Since pyramidal upsampling appeared
 to reduce errors (robustly across a number of variations we tested), we
 approximated it in the fovea by applying a gaussian filter to the foveal
 disparity estimate.
 
\end_layout

\begin_layout Standard
Figure XX shows another example of runtime and performance vs.
 foveal area.
 In this case, data cost was calculated at each pixel in the periphery,
 greatly reducing the performance difference between periphery and fovea.
 This allowed us to omit two scales in the periphery rather than one.
 
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Our system has has two main components.
 First, we used simple visual features to focus resources on the most task-relev
ant regions of the scene.
 Specifically, we weighted each pixel according to the rectified difference
 between it's disparity and it's time-averaged disparity, emphasizing pixels
 that are highly relevant for obstacle avoidance.
 This approach requires less computation than a more sophisticated saliency
 map [TODO: ref], and disregards task-irrelevant features that contribute
 to saliency.
 Importantly, it would be straightforward to switch between many such weighting
 schemes, or to mix them continuously, according to task and goal changes.
 For example, in a grasping task, large weights might be assigned to areas
 that appear to have handle-like shapes based on rapid preliminary processing
 
\begin_inset CommandInset citation
LatexCommand cite
key "TenPas2014"

\end_inset

.
 This task-oriented approach involves less computation than saliency, and
 it has more in common with human eye movements.
 
\end_layout

\begin_layout Standard
Second, we adapted a multiscale loopy belief propagation algorithm so that
 the finest scales are calculated only in the most task-relevant foveal
 regions.
 
\end_layout

\begin_layout Standard
Together, these methods allowed us to esimtate the most task-relevant surfaces
 relatively accurately, while avoiding the computational cost of accurate
 estimation across the whole frame.
 
\end_layout

\begin_layout Subsection
Relationship with Other Work
\end_layout

\begin_layout Standard
[TODO: Jeff]
\end_layout

\begin_layout Subsection
Numbers of Sub-Foveas
\end_layout

\begin_layout Standard
We found that two sub-foveas most often covered heavily-weighted areas more
 effectively than any other number up to 30.
 This outcome is a consequence of our greedy algorithm for sub-fovea placement.
 It is also related to the particular statistics of the KITTI data, and
 to our cost function.
 Nonetheless, it is interesting that larger numbers of sub-foveas (which
 are better able to cover complex shapes) were not typically much better
 than two.
 
\end_layout

\begin_layout Standard
It is also interesting that a single monolithic fovea was rarely optimal.
 The fact that the primate visual system has one fovea per eye (and that
 the eyes point to the same place) could be seen as a limitation, in that
 multiple pairs of smaller eyes could distribute processing more flexibly.
 In this context it would be interesting to estimate the cost of the single-fove
a constraint in various contexts.
 Relatedly, humans usually perform a small number of foveations per second,
 with movement times in the tens of milliseconds, whereas our method can
 move the fovea at the frame rate.
 Of course, these issues are outside the current scope.
 
\end_layout

\begin_layout Subsection
Benefits of Pyramidal Upsampling
\end_layout

\begin_layout Standard
In the periphery (i.e.
 outside the fovea), we processed images at lower resolution, and then upsampled
 the resulting depth map to the full resolution.
 We found that pyramidal upsampling greatly improved performance.
 In fact, it sometimes improved performance more than running belief propagation
 at the finest scale.
\end_layout

\begin_layout Standard
Notably, information is passed between scales in the multiscale BP algorithm
 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 by non-pyramial upsampling of the messages.
 Messages related to a pixel at one scale are simply replicated across the
 corresponding 2x2-pixel area at the next scale.
 Perhaps pyramidal upsampling of the messages would improve BP performance.
 However, this would be much more computationally intensive.
 There are four times the number of labels (128 in our case) as many messages
 as pixels, and usually about five scales, so the convolution step of pyramidal
 upsampling would be needed hundreds or thousands of times per frame.
 One might expect that belief propagation from this starting point would
 compensate for poor upsampling, but we had similar results even with ten
 or more iterations at each scale.
 
\end_layout

\begin_layout Subsection
Future Work
\end_layout

\begin_layout Standard
Our general approach is suitable for navigation in mobile robots with limited
 processing power.
 We are intersted in applying it more specifically to autonomous navigation
 among pedestrians.
 This may lead to refinement of the pixel weighting.
 For example the weights might be more directly tied to decisions that the
 robot must make, e.g.
 whether a narrow gap is too narrow to navigate.
 
\end_layout

\begin_layout Standard
Another potential direction for future work would be to adapt the method
 to parallel hardware such as a GPU or FPGA.
 The multiple-resolution version of our approach (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

) is well suited for such an implementation, because each scale has the
 same number of pixels.
 For example, if the fovea is 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels, then the immediate parafoveal region is 
\begin_inset Formula $2m$
\end_inset

 by 
\begin_inset Formula $2n$
\end_inset

, downsampled by a factor of two, for a total of 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

pixels.
 An FPGA with a capacity for 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $n$
\end_inset

 pixels could process such a frame in a few steps, potentially yielding
 high framerates with modest hardware.
 
\end_layout

\begin_layout Section*
Acknowledgements 
\end_layout

\begin_layout Standard
This work was supported in part by a Mitacs and CrossWing Inc.
 The authors thank Hamid Tizhoosh and John-Paul Gignac for helpful discussions.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "fast-stereo"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
