#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url}
%\usepackage{natbib}
\usepackage[authoryear]{natbib}
\DeclareMathOperator*{\argmin}{arg\,min}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Task-Driven Resource Allocation for Real-Time Stereo with Belief Propagation
\end_layout

\begin_layout Author
Eric Hunsberger
\begin_inset script superscript

\begin_layout Plain Layout
1
\end_layout

\end_inset

, Jeff Orchard
\begin_inset script superscript

\begin_layout Plain Layout
2
\end_layout

\end_inset

 & Bryan Tripp
\begin_inset script superscript

\begin_layout Plain Layout
1
\end_layout

\end_inset


\end_layout

\begin_layout Address
\begin_inset script superscript

\begin_layout Plain Layout
1
\end_layout

\end_inset

Systems Design Engineering, University of Waterloo, Canada
\begin_inset Newline newline
\end_inset


\begin_inset script superscript

\begin_layout Plain Layout
2
\end_layout

\end_inset

Cheriton School of Computer Science, University of Waterloo, Canada
\end_layout

\begin_layout Abstract
Stereo matching is a versatile approach to depth estimation.
 However, accurate algorithms for dense, high-resolution stereo are computationa
lly intensive, and hard to run at frame rates that are practical for robotics.
 In this paper we explore an approach that focuses computational resources
 for depth estimation on important parts of the scene.
 Specifically, we use multi-scale loopy belief propagation, but only run
 the finest scales in a small selected region that we call the 
\begin_inset Quotes eld
\end_inset

fovea
\begin_inset Quotes erd
\end_inset

 (as its purpose is analogous to the high-resolution region of the human
 retina).
 The fovea is moved in each frame to minimize a task-dependent cost function.
 Its movement is therefore determined by a combination of task parameters
 and image features.
 We evaluate this approach on stereo videos from the KITTI Vision Benchmark
 Suite, using a simple cost function that emphasizes pixels with higher-than-nor
mal disparity for their image location.
 We show that this approach can improve practical depth estimates within
 a given frame-rate constraint, and discuss analogies with task-dependent
 control of human eye movements.
\end_layout

\begin_layout Standard
Key words: stereo vision, disparity, belief propagation, fovea
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Information about the locations of surfaces in the environment is essential
 for robot navigation, manipulation, etc.
 Stereoscopic disparity (i.e.
 difference in the location of a given feature in images from two offset
 cameras) is a rich source of this information.
 Disparity is inversely related to the feature's distance (depth) from the
 cameras.
 Compared to other approaches to depth estimation, stereo is currently more
 cost effective than LIDAR, and can provide estimates from a large field
 of view at high frame rates.
 It is also more economical and higher-resolution than time-of-flight devices.
 In contrast with systems that rely on patterned infrared illumination (e.g.
 the Microsoft Kinect), it is not limited by infrared interference from
 sunlight.
 
\end_layout

\begin_layout Standard
The first step in stereo depth estimation is to match image features in
 one camera with those in the other camera.
 This is done by comparing a small region of one image with a range of horizonta
lly-offset regions in the other image.
 The most similar region in the second image (or perhaps one of the few
 most similar regions) is relatively likely to correspond to the same point
 in physical space.
 
\emph on
Sparse
\emph default
 stereo methods match only a minority of features, i.e.
 distinctive ones that are likely to be matched correctly 
\begin_inset CommandInset citation
LatexCommand cite
key "Dhond1989"

\end_inset

.
 In contrast, 
\emph on
dense
\emph default
 methods provide estimates over the whole field of view, even in featureless
 regions.
 Dense methods are of interest in robotics, because surface locations may
 affect the robot whether or not they are filled with distinctive visual
 features.
 However, in the absence of distinctive features (e.g.
 a textureless wall), a wide range of regions may match equally well.
\end_layout

\begin_layout Standard
Neighboring disparities are highly correlated except at object boundaries.
 If a visually ambiguous region is close to unambiguous regions (i.e.
 with narrow probability densities over disparity), then accounting for
 spatial correlations can profoundly improve estimates in the ambiguous
 region.
 An effective approach is to model the disparity map as a Markov random
 field.
 Then, maximum 
\emph on
a posteriori
\emph default
 depth images can be estimated using various methods, such as loopy belief
 propagation and graph cuts 
\begin_inset CommandInset citation
LatexCommand cite
key "Tappen2003,Szeliski2008"

\end_inset

.
 But despite many recent advances in efficiency, these methods remain computatio
nally intensive, limiting their practicality for robots, which require high
 frame rates with modest computational resources.
 
\end_layout

\begin_layout Standard
In this study, we explore the benefits of processing different parts of
 the scene at different levels of detail.
 This general strategy is based on the primate visual systems.
 Primates use disparity as a depth cue, but in contrast with conventional
 image processing, a disproportionately large amount of cortex is dedicated
 to the centre of the visual field (the fovea), with progressively fewer
 cortical resources available for more peripheral parts of the visual field
 
\begin_inset CommandInset citation
LatexCommand cite
key "Daniel1961"

\end_inset

.
 The eyes move frequently, typically jumping to a new target several times
 per second, to sense and analyze detail from different parts of the scene
 in series.
 Analogously, in this study we process 
\begin_inset Quotes eld
\end_inset

foveal
\begin_inset Quotes erd
\end_inset

 image regions at high resolution and 
\begin_inset Quotes eld
\end_inset

peripheral
\begin_inset Quotes erd
\end_inset

 regions at lower resolution in order to save computation time.
 
\end_layout

\begin_layout Standard
The key problem is where to position the fovea in each frame.
 Primates address this problem through sophisticated systems for directing
 visual attention and eye movements (reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Hayhoe2005,Kowler2011,Schutz2011"

\end_inset

).
 Computational models of these systems, and applications to computer vision
 and robotics, are reviewed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Borji2013,Frintrop2010,Kimura2013,Tsotsos2011"

\end_inset

.
 As discussed in these reviews, human eye movements are occasionally directed
 to salient visual features (such as the onset of motion), but in most situation
s are overwhelmingly determined by task demands.
 Examples of task-dependent targets include the next word while reading
 
\begin_inset CommandInset citation
LatexCommand cite
key "Rayner2010"

\end_inset

, the edge of an obstacle while navigating 
\begin_inset CommandInset citation
LatexCommand cite
key "Rothkopf2007"

\end_inset

, an object a person wants to pick up 
\begin_inset CommandInset citation
LatexCommand cite
key "Johansson2001"

\end_inset

, etc.
 The visual target can even be a completely featureless region.
 For example people often glance at a spot where they intend to put something,
 even though the spot may be visually indistinct 
\begin_inset CommandInset citation
LatexCommand cite
key "Tatler2011"

\end_inset

.
 However, eye movements are very often determined by a combination of bottom-up
 and top-down factors.
 
\end_layout

\begin_layout Standard
A simple example of bottom-up/top-down interaction occurs in visual search
 tasks.
 Viewing an image of many small shapes, humans can rapidly find shapes with
 a distinctive feature (e.g.
 the yellow ones; the horizontal ones; etc.) Interestingly, visual search
 for more complex conjunctions of features (e.g.
 horizontal yellow shapes) is slower and less automatic 
\begin_inset CommandInset citation
LatexCommand cite
key "Treisman1980"

\end_inset

.
\end_layout

\begin_layout Standard
Taking inspiration from the primate visual system, our goal here is to obtain
 practical real-time stereo depth estimates by allocating computational
 resources to the most task-relevant image regions in each frame.
 Taking further inspiration from the visual search literature, we direct
 the fovea using a simple visual feature that is relevant to visual navigation.
 Specifically, we use previous frames to estimate an expected disparity
 map, and define an 
\begin_inset Quotes eld
\end_inset

importance map
\begin_inset Quotes erd
\end_inset

 as the half-rectified difference between expected disparity and a time-average
 disparity (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Maki2000"

\end_inset

 for another use of depth as an attention cue).
 This allows us to direct the fovea to regions in which surfaces seem to
 be closer than normal for their part of the visual field, which often correspon
d to obstacles.
 This approach leads to improved estimation of obstacle surfaces at practical
 frame rates.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
Our general strategy is to perform fast disparity estimation over most of
 the image, with more accurate estimation in a small window, and to move
 this window each frame to minimize a task-specific cost function.
 This general strategy could be used with a variety of stereo algorithms,
 but we focus here on a specific implementation of the strategy that uses
 loopy belief propagation.
 
\end_layout

\begin_layout Subsection
Markov Random Fields and Loopy Belief Propagation
\end_layout

\begin_layout Standard
Pixel-by-pixel stereo correspondences are frequently ambiguous, so disparity
 estimation is improved by considering spatial correlations.
 The full correlation matrix is unmanageable, because disparity images of
 practical interest have tens of thousands of pixels or more.
 A successful approach 
\begin_inset CommandInset citation
LatexCommand cite
key "Sun2003"

\end_inset

 has been to model disparity images as Markov random fields.
 In a Markov random field (MRF), each region is independent of the rest
 of the field, conditional on values in the region's boundary.
 That is, 
\begin_inset Formula $p(x_{i}|x_{b},x_{o})=p(x_{i}|x_{b})$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is the part of the field inside the boundary, 
\begin_inset Formula $x_{b}$
\end_inset

 is the part that comprises the boundary, and 
\begin_inset Formula $x_{o}$
\end_inset

 is the part outside the boundary 
\begin_inset CommandInset citation
LatexCommand cite
key "Fieguth2010"

\end_inset

.
 There are various ways to estimate the maximum 
\emph on
a posteriori
\emph default
 disparity from the MRF.
 
\end_layout

\begin_layout Standard
One such method is belief propagation (BP), a general algorithm for inference
 on graphical models (including Bayesian networks as well as MRFs).
 BP provides exact solutions on trees.
 Loops in the statistical relationships of Markov random fields prevent
 exact inference, but 
\begin_inset Quotes eld
\end_inset

loopy
\begin_inset Quotes erd
\end_inset

 BP typically produces good approximations after running for a few iterations
 
\begin_inset CommandInset citation
LatexCommand cite
key "Murphy1999"

\end_inset

.
 
\end_layout

\begin_layout Standard
The starting point for our work is a BP implementation by Felzenszwalb &
 Huttenlocher 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 that has several optimizations, which together accelerate the algorithm
 by several orders of magnitude.
 One of these optimizations is a multi-scale method that reduces the numbers
 of iterations needed to propagate information to distant parts of the image.
 Our modification consists simply of executing finer scales (which take
 up most of the computation time) only in sub-images rather than over the
 whole image.
 
\end_layout

\begin_layout Standard
The BP implementation of 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 results in a labelling 
\begin_inset Formula $f$
\end_inset

, which assigns a label 
\begin_inset Formula $f_{p}\in\mathcal{L}$
\end_inset

 to pixel 
\begin_inset Formula $p$
\end_inset

, where 
\begin_inset Formula $\mathcal{L}$
\end_inset

 consists of all possible disparities.
 The algorithm minimizes an energy function 
\begin_inset Formula 
\[
E(f)=\sum_{p\in\mathcal{P}}D_{p}(f_{p})+\sum_{p,q\in\mathcal{N}}V(f_{p}-f_{q})
\]

\end_inset

where 
\begin_inset Formula $D_{p}$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

data cost
\begin_inset Quotes erd
\end_inset

) is the cost of labelling pixel 
\begin_inset Formula $p$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

, and 
\begin_inset Formula $V(f_{p}-f_{q})$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

discontinuity cost
\begin_inset Quotes erd
\end_inset

) is an additional cost of labelling neighboring pixels 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 as 
\begin_inset Formula $f_{p}$
\end_inset

 and 
\begin_inset Formula $f_{q}$
\end_inset

.
 Because the disparity of neighboring pixels is strongly correlated, a larger
 discontinuity cost is assigned for larger differences between neighbors.
 Minimizing 
\begin_inset Formula $E(f)$
\end_inset

 corresponds to maximum 
\emph on
a posteriori
\emph default
 estimation in a probabilistic context 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

.
 
\end_layout

\begin_layout Standard
BP involves iterative computation and exchange of messages between pixels.
 In iteration 
\begin_inset Formula $t$
\end_inset

, the message 
\begin_inset Formula $m_{p\to q}^{t}(f_{q})$
\end_inset

 from pixel 
\begin_inset Formula $p$
\end_inset

 to pixel 
\begin_inset Formula $q$
\end_inset

 for disparity 
\begin_inset Formula $f_{q}$
\end_inset

 is 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset


\begin_inset Formula 
\[
m_{p\to q}^{t}(f_{q})=\min_{f_{p}}\left(V(f_{p}-f_{q})+D_{p}(f_{p})+\sum_{s\in\mathcal{N}(p)\backslash q}m_{s\to p}^{t-1}(f_{p})\right)
\]

\end_inset

where 
\begin_inset Formula $\mathcal{N}(p)\backslash q$
\end_inset

 consists of the neighbors of 
\begin_inset Formula $p$
\end_inset

 other than 
\begin_inset Formula $q$
\end_inset

.
 After 
\begin_inset Formula $T$
\end_inset

 iterations, final labels 
\begin_inset Formula $f_{p}^{*}$
\end_inset

 are assigned as 
\begin_inset Formula 
\[
f_{p}^{*}=\argmin_{f_{p}}\left(D(f_{p})+\sum_{q\in\mathcal{N}(p)}m_{q\to p}^{T}(f_{p})\right).
\]

\end_inset

 
\end_layout

\begin_layout Standard
In stereo estimation, the first step in this process is to calculate a data
 cost volume in terms of image coordinates 
\begin_inset Formula $p=(x,y)$
\end_inset

 (where 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are the horizontal and vertical image coordinates, respectively), and disparity
 (in pixels) 
\begin_inset Formula $d$
\end_inset

.
 The data cost volume is specifically 
\begin_inset Formula 
\[
D_{x,y,d}=\min\left\{ \left|I_{x,y}^{l}-I_{x-d,y}^{r}\right|,D_{\mathrm{max}}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $I^{l}$
\end_inset

 and 
\begin_inset Formula $I^{r}$
\end_inset

 are luminance of the left and right images, and 
\begin_inset Formula $D_{\mathrm{max}}$
\end_inset

 is a saturation value that limits the cost of large discontinuities.
 The saturation value is used because while most discontinuities are expected
 to be small, some (at object boundaries) are expected to be larger, with
 no particular expectation about how much larger.
 
\end_layout

\begin_layout Standard
Prior to calculating the data cost, we process the images with a Laplacian
 filter to emphasize edges.
 
\end_layout

\begin_layout Standard
We refer to the disparity estimated from BP in frame 
\begin_inset Formula $k$
\end_inset

 as 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 .
\end_layout

\begin_layout Subsection
System Overview
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:motivation"

\end_inset

 illustrates the motivation for our approach.
 The two curves show error in stereo-disparity estimation performed at two
 different image resolutions.
 To perform disparity estimation at a lower resolution, the images are downsampl
ed, then belief propagation is performed, and the result is upsampled.
 Performing belief propagation at a lower resolution decreases runtime and
 increases error.
 For a given resolution, error decreases with each iteration of belief propagati
on (BP), but most of this decrease occurs in the first few iterations.
 Beyond 5-10 iterations, error can only be reduced substantially by switching
 to a higher resolution (e.g.
 by switching from the left curve to the right curve).
 
\end_layout

\begin_layout Standard
Suppose we have a certain time budget per frame (e.g.
 1/4s) and the right curve is entirely outside it.
 In this case, we can only afford to process the full frame at lower resolution.
 However, it may be possible to reduce error somewhat below the left curve
 by processing only part of each frame at higher resolution.
 This part can be whatever size uses all the available time.
 Furthermore, in many applications (e.g.
 navigation, grasping), some areas in each frame are likely to be more important
 than others.
 If the most important areas are processed at higher resolution, then the
 runtime may remain acceptable while the importance-weighted error approaches
 that of higher-resolution BP.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-rationale-no-saturation.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Performance of multi-scale belief propagation on example data, illustrating
 the motivation for our approach.
 Disparity estimation error is plotted vs.
 runtime.
 The right curve (circles) were obtained with a high-resolution stereo image
 pair, and the left curve (squares; higher error) with lower-resoluton versions
 of the same images.
 Within each curve, the runtime varies with numbers of iterations at each
 scale (1, 2, 3, 4, 5, 7, 10, and 15 iterations).
 Given a time budget of, for example, 0.25s/frame, it would not be possible
 to process these images at high resolution.
 However, if certain areas in the images were of greater practical interest,
 then results that are nearly as useful might be achieved by processing
 just those areas at high resolution.
 The data are 50 frames from the KITTI dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

, downsampled by a factor of two (high resolution) and four (low resolution)
 in each dimension.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:motivation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our system performs multi-scale belief propagation (BP) with different numbers
 of scales in different parts of the image.
 The number of operations in a single scale of Felzenszwalb & Huttenlocher's
 method 
\begin_inset CommandInset citation
LatexCommand cite
key "Felzenszwalb2006"

\end_inset

 is order 
\begin_inset Formula $O(nl)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of pixels and 
\begin_inset Formula $l$
\end_inset

 is the number of disparities.
 The majority of the computational cost is incurred at the finest scale,
 which has four times as many pixels as the next-finest scale.
 Our system only processes the finest one or two scales in the foveal region.
 The resulting depth estimate is similar to that of full multi-scale BP
 in the fovea, but (depending on fovea size) it can run nearly as fast as
 if the finest scales are omitted.
 
\end_layout

\begin_layout Standard
Importantly, messages from coarser scales are used to initialize messages
 in finer, foveal scales.
 This allows information from coarse scales to propagate from other image
 regions across the fovea, which makes depth estimates in the fovea continuous
 with those in the surrounding areas, and also makes them similar to those
 of full-resolution BP.
 (They differ somewhat around the fovea border, due to propagation across
 the border in the fine scale of full BP.) 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:system"

\end_inset

 illustrates our general approach.
 The inputs to the system in each frame are: 1) a rectified stereo pair
 of luminance images, 2) the position of the fovea, and 3) parameters of
 the cost function (described in the next section).
 The latter would normally be static throughout a task, although this is
 not essential.
 The outputs after processing each frame are: 1) a disparity map, and 2)
 the position of the fovea for the next frame.
\end_layout

\begin_layout Standard
Downsampled copies of the disparity estimate 
\begin_inset Formula $D_{k}$
\end_inset

 are stored for several frames to allow integration of information over
 time.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename system-sketch.pdf
	scale 65
	BoundingBox 130bp 150bp 590bp 390bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
General structure of our approach.
 The 2D boxes correspond to single-channel luminance and disparity images.
 The thicker boxes indicate multichannel images, including the data cost
 over 128 disparities, and optional retention of one or more previous frames.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:system"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
The methods were tested on data from the 2012 KITTI Vision Benchmark Suite
 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

.
 This dataset includes high-resolution rectified stereo images (1242x375
 pixels) taken at ten frames/s from the roof of a car during city driving.
 Rectified images and odometry are available for a number of long sequences.
 Ground truth depth (but not odometry) is available for single frames within
 a number of short 20-frame sequences.
 Ground truth is from a LIDAR sensor.
 The ground-truth points are fairly dense, because points were combined
 from several sweeps of the sensor using a semi-manual registration process.
 The scenes include static obstacles such as buildings and parked cars,
 and moving obstacles such as cars, pedestrians, and people on bicycles.
 
\end_layout

\begin_layout Subsection
Fovea Placement
\end_layout

\begin_layout Standard
Fovea placement was task-dependent.
 In general, it was intended to minimize task-specific cost functions of
 the form, 
\begin_inset Formula 
\[
C_{x,y}=\sum_{x,y}w_{x,y}\min\left(\left|d_{x,y}^{*}-d_{x,y}\right|,C_{\mathrm{sat}}\right),
\]

\end_inset

where 
\begin_inset Formula $d_{x,y}$
\end_inset

 is ground-truth disparity, 
\begin_inset Formula $w_{x,y}$
\end_inset

 is a weight map that has large values in the most task-relevant regions,
 and pixel-wise errors saturate at 
\begin_inset Formula $C_{\mathrm{sat}}$
\end_inset

.
 Weights 
\begin_inset Formula $w_{x,y}$
\end_inset

 depend on the task and also on a preliminary disparity estimate 
\begin_inset Formula $d_{x,y,k}^{0}$
\end_inset

, which is either obtained by remapping past frames (when odometry was available
; see next section) or by running BP only at coarse scales.
 
\end_layout

\begin_layout Standard
One possible weighting scheme for navigation is to emphasize regions in
 the direction of travel, because the risk of collisions with obstacles
 is greatest in this direction.
 Another is to emphasize regions in which surfaces are relatively close
 (i.e.
 disparity is high).
 However, both of these schemes emphasize the close parts of the ground,
 which is often clear of obstacles.
 Instead, we define 
\begin_inset Formula 
\[
w_{x,y}^{d}=\max(d_{x,y}^{0}-\bar{d}_{x,y}-d_{\mathrm{th}},0),
\]

\end_inset

where 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 is the mean of 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

 over time, and 
\begin_inset Formula $d_{\mathrm{th}}$
\end_inset

 is a small threshold.
 This approach emphasizes parts of the image in which surfaces are closer
 than usual.
 We used 
\begin_inset Formula $w_{x,y}=w_{x,y}^{d}w_{x,y}^{s}$
\end_inset

, where 
\begin_inset Formula $w_{x,y}^{s}$
\end_inset

 is a static weight template that is highest in the horizontal centre of
 the image (the direction of travel) and also lower at the top of the image
 than the bottom.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:weighting-example"

\end_inset

 shows an example frame from the KITTI dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "Geiger2012"

\end_inset

 in which a cyclist is strongly emphasized by this method.
 
\end_layout

\begin_layout Standard
For simplicity, we calculated 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 over full videos before processing, which is unrealistic for deployment
 on a robot.
 However, a recursive low-pass filter would have a similar effect with small
 computational cost.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/importance-tight.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Weighting by disparity in excess of the average.
 Top: Average disparity 
\begin_inset Formula $\bar{d}_{x,y}$
\end_inset

 estimated over a long image sequence.
 Centre: Disparity 
\begin_inset Formula $d_{x,y}^{*}$
\end_inset

 estimated in a single frame.
 Bottom: The weight 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

, where 
\begin_inset Formula $w^{s}$
\end_inset

 is a static weight template that emphasizes the direction of travel, and
 
\begin_inset Formula $w_{x,y}^{d}=\max(d_{x,y}^{*}-\bar{d}_{x,y},0)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset CommandInset label
LatexCommand label
name "fig:weighting-example"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The fovea is positioned so that it covers the region with the highest total
 
\begin_inset Formula $w_{x,y}$
\end_inset

, calculated efficiently using an integral image.
 This approach minimizes the cost if the error is statistically uniform
 over the image, and lower within the fovea than outside the fovea.
 
\end_layout

\begin_layout Standard
Contrary to the example of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:weighting-example"

\end_inset

, the high-weight pixels are not always co-located.
 Therefore we also experimented with dividing the fovea into multiple sub-foveas
 with the same total number of foveal pixels.
 Whereas optimal placement of a single fovea with given height and width
 simply corresponds to finding the maximum in a two-dimensional image, it
 is not practical to find the global optimal placement of multiple foveas.
 Instead, we used a greedy approach in which we placed a single sub-fovea
 optimally, set the weight within it to zero, then placed another single
 sub-fovea optimally, etc.
 We calculated the total remaining weight with different numbers of sub-foveas,
 and used the number of sub-foveas with the lowest weight.
 These computations were all performed with images that were downsampled
 several times, so they were not computationally intensive.
 
\end_layout

\begin_layout Subsection
Remapping Past Frames
\end_layout

\begin_layout Standard
If odometry is available, the preliminary disparity estimate needed for
 fovea placement for frame 
\begin_inset Formula $k$
\end_inset

 can be obtained by mapping the final estimate 
\begin_inset Formula $d_{k-1}^{*}$
\end_inset

 from the previous frame into the coordinates of the new frame.
 This mapping consists of converting the disparities from frame 
\begin_inset Formula $k-1$
\end_inset

 to depth, then to 3D position in the left camera's coordinate system in
 frame 
\begin_inset Formula $k-1$
\end_inset

, then 3D position in the left camera's coordinate system in frame 
\begin_inset Formula $k$
\end_inset

, and finally to depth and disparity from the frame-
\begin_inset Formula $k$
\end_inset

 perspective.
 
\end_layout

\begin_layout Standard
Due to changes in perspective, there is not a one-to-one map between pixels
 in frames 
\begin_inset Formula $k-1$
\end_inset

 and 
\begin_inset Formula $k$
\end_inset

, so remapped disparity images contain blank pixels.
 We experimented with two ways of filling in the blank pixels.
 The first was linear interpolation, and the second was setting blank pixels
 to zero and taking the maximum over small neighborhoods.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:interp"

\end_inset

 compares these two methods.
 Taking the maximum was much faster than bilinear interpolation, with comparable
 results, so we used the maximum.
 
\end_layout

\begin_layout Standard
We also experimented with using remapped disparity estimates from previous
 frames to improve current-frame estimates.
 Specifically, we augmented the data cost to penalize changes over time.
 In some cases this method removed artifacts.
 However, we were unable to clearly evaluate whether this strategy improved
 performance on average, because the KITTI dataset does not include odometry
 and ground-truth disparity for the same image sequences.
 Therefore we simply note that this is a potential additional use for remapped
 disparity.
 Relatedly, belief propagation can also be performed over time as well as
 image dimensions 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhu2010"

\end_inset

 although this is more computationally intensive.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/smudge-vs-interp.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of interpolation methods for remapped data.
 The top-left panel is a disparity map 
\begin_inset Formula $d_{x,y,k}^{*}$
\end_inset

.
 The top-right is the previous frame's estimate, 
\begin_inset Formula $d_{x,y,k-1}^{*}$
\end_inset

, remapped into the current frame's perspective.
 Aside from the missing data (dark blue points), this is a fair approximation
 of the current disparity estimate (importantly, nothing in this scene is
 moving except the cameras).
 The bottom-right panel shows bilinear interpolation of the remapped data.
 The bottom-left panel is the maximum of the remapped image and copies of
 this image shifted one pixel vertically and one pixel both left and right.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:interp"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parameters
\end_layout

\begin_layout Standard
The system parameters and their values for different tests are listed in
 Table
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:parameters"

\end_inset

.
 Optimal parameters were found using the package Hyperopt 
\begin_inset CommandInset citation
LatexCommand cite
key "Bergstra2013"

\end_inset

.
 Hyperopt attempts to remove the irreproducible ``art'' of hand-tuning hyperpara
meters, instead using an automated method for searching hyperparameter space
 to find near-optimal values.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
System parameters and values used in performance tests.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Laplacian filter width
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3 pixels
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data-cost weight
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.014
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data-cost saturation value (
\begin_inset Formula $C_{max}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
112
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Discontinuity cost saturation value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BP iterations per scale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:parameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

 shows two disparity estimates from a single frame, with the fovea in different
 places (the centres are marked with white + signs).
 These examples were processed with multiple levels of resolution, with
 a small high-resolution region in the centre and multiple levels of decreasing
 resolution with greater distance from the centre, analogous to the human
 eye and visual cortex.
 The total runtime of BP in these examples is only a few times as great
 as BP at the lowest resolution.
 In the top frame, the fovea clearly resolves the cyclist, while in the
 bottom frame the fovea correctly interprets a low-disparity region in which
 the top frame has an artifact.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fovea-examples-tight.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of stereo disparity estimation with foveal multi-scale belief propagati
on.
 The white + marks indicate fovea centres.
 These examples were processed at multiple resolutions, with equal computational
 demands at each of these resolutions (e.g.
 a parafoveal region twice the size of the fovea had half the resolution).
 In the top panel, the fovea resolves a cyclist at high resolution.
 There is a large artifact in the top-centre of the image.
 In the bottom panel (same frame), the fovea is instead centred on this
 artifact, and corrects it.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:fovea-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:foveation-sequence"

\end_inset

 shows a sequence of foveations while driving between parked cars.
 The sequence is from top to bottom.
 In this example a single fovea is used (i.e.
 without multiple sub-foveas).
 The cars on the right are foveated in turn.
 The cars on the left have slightly lower weights than those on the right,
 and are never foveated.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/foveation-sequence-tight.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example foveation sequence.
 The fovea boundary is shown as a white square, and the frames are ordered
 from top to bottom.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:foveation-sequence"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:numbers-of-subfoveas"

\end_inset

 shows examples of sub-fovea placement, overlaid with weighting functions.
 In the top image, the fovea has been broken into 30 sub-foveas that are
 placed individually.
 A greater number of smaller foveas can cover irregular shapes more closely.
 However, because our greedy method of sub-fovea placement is not globally
 optimal, more sub-foveas are not guaranteed to cover a larger cost.
 In this example, numbers of sub-foveas from 1-30 were compared (with the
 same total foveal pixels in each case), and two sub-foveas provided the
 best coverage (bottom image).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/multiple-foveas-30-vs-best.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of greedy-optimal placement of 30 sub-foveas (top) and 2 sub-foveas
 (bottom), with the same total area, overlaid with the corresponding weight
 image 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

.
 In the weight image, darker blue corresponds to low weight.
 In this example, two sub-foveas provided the best coverage of weighted
 pixels, compared to options of between 1 and 30 sub-foveas.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:numbers-of-subfoveas"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Sub-Fovea Selection
\end_layout

\begin_layout Standard
The time taken by our greedy method of sub-fovea placement is roughly linear
 in the number of sub-foveas, and the time to compare all of 
\begin_inset Formula $\{1,2,..n\}$
\end_inset

 sub-foveas is quadratic.
 Although checking more possibilities will generally lead to better performance,
 it is inefficient to check unlikely possibilities.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:num-subfoveas-histogram"

\end_inset

 shows a histogram of the numbers of sub-foveas with the best weight coverage
 over a set of example frames, up to a maximum of 30.
 The total foveal area was 20% of the image area.
 Two sub-foveas were often best.
 Similar results were obtained with total foveal area 10% of the image.
 Two sub-foveas were also most frequently best if the total foveal area
 took up 40% of the image, but in this case a single sub-fovea was best
 nearly as often as two, and larger numbers of foveas were rarely best.
 In further experiments we used the best of 1-5 sub-foveas.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/num-fovea-hist-20-percent.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Frequency with which different numbers of foveas covered the greatest integrated
 weight in weight images 
\begin_inset Formula $w=w^{s}w^{d}$
\end_inset

.
 This histogram is based on all 194 stereo image pairs from the KITTI training
 dataset, with total foveal area 20% of the frame area.
  
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:num-subfoveas-histogram"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accuracy vs.
 Run Time
\end_layout

\begin_layout Standard
We tested the dependence of both runtime and accuracy on the total foveal
 area.
 In general, our expectations were that runtime would increase almost linearly
 with the foveal area, and that accuracy would improve with increasing foveal
 area.
 For example, with zero foveal area, speed and accuracy should be nearly
 equal to that of full BP with down-sampled images.
 On the other hand, setting the fovea area equal to the image area should
 result in speed and accuracy equal to that of full multi-scale BP.
 We further anticipated that weighted error would drop sharply with increasing
 foveal area, since we tried to align the fovea with the highest-weight
 regions.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/performance-fovlevels2-minlevel2-sat20-coarse-peripheral-data-cost.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of performance as a function of fovea size.
 The fovea size (horizontal axis) varies as a fraction of the total image
 size, from 0 to 1.
 The top panel shows unweighted error based on LIDAR-based ground-truth
 disparity.
 The error measure is the mean absolute difference between estimate and
 ground truth, with the difference in each pixel clipped at a maximum of
 20 pixels, to de-emphasize outliers.
 The blue line corresponds to a single fovea centred on the image (i.e.
 not moved to high-weight regions).
 The green line shows the same error with a single mobile fovea, and the
 red shows the same error with up to five mobile sub-foveas.
 The centre panel is identical except that the error is weighted, with mean
 weight normalized to one.
 Moving a single fovea to high-weight regions made the error drop more sharply
 with increasing foveal area (green vs.
 blue line), and more so when up to five sub-foveas were allowed (red line).
 The weighted error was higher overall than the unweighted error, reflecting
 the fact that our weighting scheme emphasized more challenging regions.
 The bottom panel shows how run time increased with fovea size.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:performance-coarse-data-cost"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:performance-coarse-data-cost"

\end_inset

 shows an example of results that are in keeping with these expectations.
 In order to achieve a range of runtimes consistent with the frame rate
 of the KITTI data (10Hz), the full images (roughly 
\begin_inset Formula $1242\times375$
\end_inset

 pixels with minor variations) were downsampled by a factor of two, the
 finest scale of belief propagation was omitted in the fovea, and the two
 finest scales were omitted in the periphery.
 
\end_layout

\begin_layout Standard
The horizontal axis is the total foveal area as a fraction of image area.
 When we held the fovea at the centre of the image (blue line), error dropped
 roughly linearly as a function of foveal area.
 In contrast, weighted error (centre panel) dropped more steeply when a
 single fovea was aligned with high-weight regions, and more steeply still
 when several sub-foveas were allowed.
 The run time with zero foveal area includes both the calculation of data
 cost, and belief propagation at the coarser scales.
 To reduce the time needed to calculate the data cost in the periphery,
 the cost was calculated on subsampled pixels, at the resolution of the
 peripheral belief propagation.
 
\end_layout

\begin_layout Standard
Performance was generally poorer in high-weight regions, i.e.
 around nearby obstacles, than in the more smoothly-changing background.
 Performance in these regions was also quite variable from frame to frame
 (not shown).
 
\end_layout

\begin_layout Standard
The run times (bottom panel) are mean total times per frame (for calculation
 of the data cost, belief propagation, and inference) on a MacBook Pro with
 a 2.5GHz Intel Core i7 processor.
 Additional time for fovea selection was about 
\begin_inset Formula $50\mu s$
\end_inset

.
 
\end_layout

\begin_layout Standard
We found that the weighted errors were sensitive to the way we calculated
 the data cost.
 Superior results were obtained by calculating the data cost at each pixel
 (i.e.
 same resolution as the fovea), and downsampling by summing the results
 across windows of 
\begin_inset Formula $2\times2$
\end_inset

 pixels, with a slight increase in runtime.
 
\end_layout

\begin_layout Standard
To put these results in context, we evaluated 
\emph on
unweighted
\emph default
 errors against the KITTI 2012 stereo benchmark test data.
 Average absolute errors, and percentages of pixels with >3 pixels error,
 are shown in Table 2 for both 0% fovea and 100% fovea.
 At the time of submission, the 0% and 100% fovea results correspond to
 83rd-place and 70th-place rankings on the KITTI benchmark in terms of the
 percentage of unoccluded pixels with greater than 3-pixels error.
 However, the methods ranked 60-69 have an average runtime of 80s per frame,
 with a range of 0.3s to 242s.
 These results are not directly related to task-weighted error (our main
 interest), but they provide context in terms of a standard benchmark.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Results on the 2012 KITTI Stereo Benchmark.
 This benchmark does not consider differences in task-relevance across the
 scene, but these results help to contextualize the underlying methods within
 the current state of the art.
 The Out-Noc column shows the percentage of unoccluded pixels with >3-pixel
 error.
 The Out-All column is the same except that it includes occluded pixels.
 Avg-Noc and Avg-All are the mean absolute errors (in pixels) over the unocclude
d pixels and over all pixels, respectively.
 The runtime is per frame on a single core of a MacBook Pro (2.5 GHz Intel
 Core i7).
 Slight improvements on the accuracy of these results were obtained by gaussian
 filtering of the disparity estimates.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="6">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fovea
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Out-Noc
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Out-All
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Avg-Noc
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Avg-All
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Runtime
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
None
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.16%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
15.90%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.9px
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.4px
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.062s
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Whole Frame
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9.35%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11.21%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.1px
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.7px
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.188s
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Counter-intuitively, given the improved calculation of data cost, belief
 propagation at the finest scale sometimes increased the 
\emph on
weighted
\emph default
 error.
 However, this quirk allowed us to improve runtime by omitting an additional
 scale of belief propagation.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:performance-fine-peripheral-cost"

\end_inset

 shows an example (with improved data cost) in which the three finest scales
 of belief propagation were omitted in the periphery, and the two finest
 were omitted in the fovea.
 Although the data cost calculation took longer in this case, processing
 could nonetheless be completed at 10 frames/s with a larger fovea (roughly
 60% vs.
 20%), and about 7% lower error.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/performance-fovlevels3-minlevel2-sat20.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
As figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:performance-coarse-data-cost"

\end_inset

, with two differences: 1) In this case, the peripheral data cost was calculated
 at each pixel and averaged to the peripheral resolution, whereas in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:performance-coarse-data-cost"

\end_inset

 the data cost was only calculated once per pixel at the peripheral resolution,
 by skipping some of the pixels at the full resolution; 2) an additional
 belief propagation scale was omitted in both the fovea and the periphery.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:performance-fine-peripheral-cost"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Our system has has two main components.
 First, we used simple visual features to focus resources on the most task-relev
ant regions of the scene.
 Specifically, we weighted each pixel according to the rectified difference
 between its disparity and its time-averaged disparity, emphasizing pixels
 that are highly relevant for obstacle avoidance.
 This weighting scheme requires less computation than a more sophisticated
 saliency map 
\begin_inset CommandInset citation
LatexCommand cite
key "Itti2000"

\end_inset

, and disregards task-irrelevant features that contribute to saliency.
 Importantly, it would be straightforward to switch between multiple such
 weighting schemes, or to mix them continuously, according to task and goal
 changes.
 For example, in a grasping task, large weights might be assigned to areas
 that appear to have handle-like shapes (suggesting grasp affordances) based
 on rapid preliminary processing 
\begin_inset CommandInset citation
LatexCommand cite
key "TenPas2014"

\end_inset

.
 As discussed in the Introduction, such a task-oriented approach has much
 in common with human eye movements.
 
\end_layout

\begin_layout Standard
The second main component of our system is an elaboration of the multi-scale
 loopy belief propagation algorithm that calculates the finest scales only
 in the most task-relevant regions.
 
\end_layout

\begin_layout Standard
Together, these methods allow us to estimate the most task-relevant surfaces
 relatively accurately, while avoiding the computational cost of accurate
 estimation across the whole frame.
 
\end_layout

\begin_layout Subsection
Numbers of Sub-Foveas
\end_layout

\begin_layout Standard
We found that two sub-foveas most often covered heavily-weighted areas more
 effectively than any other number up to 30.
 This was consistently the case across a wide range of total foveal areas.
 This outcome is a consequence of our greedy algorithm for sub-fovea placement.
 It is also related to the particular statistics of the KITTI data, and
 to our cost function.
 Nonetheless, it is interesting that larger numbers of sub-foveas (which
 are better able to cover complex shapes) were not typically much better
 than two.
 
\end_layout

\begin_layout Standard
It is also interesting that a single monolithic fovea was rarely optimal.
 The fact that the primate visual system has one fovea per eye (and that
 the eyes point to the same place) could be seen as a limitation, in that
 multiple pairs of smaller eyes could distribute processing more flexibly.
 In this context it would be interesting to estimate the cost of the single-fove
a constraint in various contexts.
 Relatedly, humans usually perform a small number of foveations per second,
 with movement times in the tens of milliseconds, whereas our method can
 move the fovea at the frame rate.
 Of course, these issues are outside the current scope.
\end_layout

\begin_layout Subsection
Relationship with Other Work
\end_layout

\begin_layout Standard
Many related stereo vision systems have been developed and implemented,
 including other stereo methods that have been applied to the KITTI benchmark,
 and other navigation-related applications.
 As one example, a stereo system was used to observe objects in a driving
 scenario 
\begin_inset CommandInset citation
LatexCommand cite
key "Bertozzi1998"

\end_inset

.
 The left and right road images were spatially rectified and compared; mismatche
s between the images indicated an object.
 However, these methods perform depth estimation on the entire image, and
 do not use the strategy of foveation to improve accuracy for a fixed computatio
nal budget.
\end_layout

\begin_layout Standard
A number of camera systems have been developed for stereo-vision applications.
 For example, 
\begin_inset CommandInset citation
LatexCommand cite
key "Kuniyoshi1996"

\end_inset

 developed a two-camera system with foveated lenses.
 The lenses had a standard projection in the fovea, but a spherical projection
 in the periphery.
 The resulting mapping of the visual scene has some mathematical advantages.
 A detailed design for an active-vision system with two cameras was presented
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Samson2006"

\end_inset

.
 These ``active vision'' systems focus on the design of the camera systems
 themselves, and not on the depth-estimation aspect of vision.
\end_layout

\begin_layout Standard
One foveation method was developed as an efficient alternative to using
 dense stereo processing 
\begin_inset CommandInset citation
LatexCommand cite
key "Klarquist1998"

\end_inset

.
 It uses two vergent cameras, with resolution that drops off in steps as
 you move away from the image centre (from fovea to periphery).
 It constructs a depth map (composed of an array of planar patches) from
 multiple fixations.
 New peripheral patches overwrite older ones, and foveal patches overwrite
 peripheral patches.
 To choose the next fixation point, a probabilistic map is produced using
 (possibly multiple) criteria, and the next fixation point is randomly chosen
 from that distribution.
\end_layout

\begin_layout Standard
Another depth estimation system used variable resolution cameras 
\begin_inset CommandInset citation
LatexCommand cite
key "Bernardino2002"

\end_inset

.
 They developed a Baysean-based stereo depth method for cameras that have
 log-polar resolution profiles.
 However, the location of their fovea is fixed and not chosen to minimize
 error or computational load.
\end_layout

\begin_layout Standard
Two methods on the KITTI 2012 leaderboard outperformed our benchmark results
 (for unweighted error) in terms of both accuracy and runtime.
 The first (Toast2) is related to 
\begin_inset CommandInset citation
LatexCommand cite
key "Ranft2014"

\end_inset

.
 This method uses block matching and the Census Transform, along with image
 shears, to infer the depth of slanted planar surfaces in the scene 
\begin_inset CommandInset citation
LatexCommand cite
key "Ranft2014"

\end_inset

.
 It would be interesting to see whether our importance-weighting approach
 could be combined with this stereo method.
 However, because this method fits image blocks to planes, it is possible
 that its better average performance corresponds to low error on large flat
 surfaces such as roads and walls, with lesser advantages for typical obstacles.
 It is therefore uncertain whether our weighted error could be improved
 with this method.
 It is also unclear how the smaller aperture of a fovea might impact its
 accuracy.
\end_layout

\begin_layout Standard
The other methd (DNC1D) is, at the time of writing, an anonymous submission
 pending CVPR 2016.
 It may be worthwhile to explore combinations of our general approach with
 DNC1D, or other stereo methods that appear in coming years.
 
\end_layout

\begin_layout Subsection
Future Work
\end_layout

\begin_layout Standard
One potential direction for future work is to adapt the method to parallel
 hardware such as a GPU or FPGA.
 The multiple-resolution version of our approach (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fovea-examples"

\end_inset

) is well suited for such an implementation, because each scale has the
 same number of pixels.
 For example, if the fovea is 
\begin_inset Formula $m\times n$
\end_inset

 pixels, then the immediate parafoveal region is 
\begin_inset Formula $2m\times2n$
\end_inset

, downsampled by a factor of two, for a total of 
\begin_inset Formula $m\times n$
\end_inset

 pixels.
 An FPGA with a capacity for 
\begin_inset Formula $m\times n$
\end_inset

 pixels could process such a frame in a few steps, yielding high frame rates
 with modest hardware.
 
\end_layout

\begin_layout Standard
Our general approach is suitable for navigation in mobile robots with limited
 processing power.
 We are interested in applying it more specifically to autonomous robot
 navigation among pedestrians.
 This may allow us to refine the pixel weighting.
 For example, the weights might be more directly tied to decisions the robot
 must make, e.g.
 whether a narrow gap is too narrow to navigate.
 
\end_layout

\begin_layout Section*
Acknowledgements 
\end_layout

\begin_layout Standard
This work was supported in part by a Mitacs and CrossWing Inc.
 The authors thank Hamid Tizhoosh, John Zelek, and John-Paul Gignac for
 helpful discussions.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "fast-stereo"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
